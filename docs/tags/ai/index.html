<!doctype html><html lang=en-us data-theme><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><link rel=preload href=../../unifont-small.woff2 as=font type=font/woff2 crossorigin=anonymous><meta charset=utf-8><meta name=HandheldFriendly content="True"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer-when-downgrade"><a rel=me href=https://fosstodon.org/@jarbus></a><title>Ai -</title><meta name=description content><link rel=alternate type=application/rss+xml href=//localhost:1313/tags/ai/index.xml title><link rel=icon type=image/png href=//localhost:1313/favicon.png><link rel=apple-touch-icon-precomposed href=//localhost:1313/favicon.png><style>body{visibility:hidden;opacity:0}</style><noscript><style>body{visibility:visible;opacity:1}</style></noscript><link rel=stylesheet href=//localhost:1313/css/style.4b4e7ad7887fe44641cc7dd0ac64187c09f60243e973b85799d3dcf4f1c42da1.css integrity="sha256-S05614h/5EZBzH3QrGQYfAn2AkPpc7hXmdPc9PHELaE="><script src=//localhost:1313/js/script.32e751300d2d69e2cb56a98d2c9d964d54a53a8fb7281ccdbd80f055c88e8d86.js type=text/javascript integrity="sha256-MudRMA0taeLLVqmNLJ2WTVSlOo+3KBzNvYDwVciOjYY="></script><script src=//localhost:1313/js/script.4a16e98326ac7d578fd6cdf11f605726c5340c174cbac62e97fdad653bc80249.js type=text/javascript integrity="sha256-ShbpgyasfVeP1s3xH2BXJsU0DBdMusYul/2tZTvIAkk="></script><script src=//localhost:1313/js/script.eaec673e30fac6ce6b490d5a7e48d4675934cc78338c36ff8df3d6ff20b87f1a.js type=text/javascript integrity="sha256-6uxnPjD6xs5rSQ1afkjUZ1k0zHgzjDb/jfPW/yC4fxo="></script><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}},window.addEventListener("load",e=>{document.querySelectorAll("mjx-container").forEach(function(e){e.parentElement.classList+="has-jax"})})</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script></head><body><a class=skip-main href=#main>Skip to main content</a><div class=container><header class=common-header><div class=header-top><h1 class=site-title><a href=//localhost:1313/></a></h1></div><img src=../../dithered-tree-contrast.gif alt="Dithered Tree" style="display:block;margin:0 auto;max-height:20vh"><nav><a href=//localhost:1313/ title=home>~jarbus</a>
<a href=//localhost:1313/blog/ title=posts>posts</a>
<a href=//localhost:1313/list title=list>list</a>
<a href=//localhost:1313/index.xml title=Rss>rss</a></nav></header><main id=main tabindex=-1><h1>Tag: Ai</h1><div class=post-info><a href=../../tags/>To all tags</a></div><article class="post-list h-feed"><div class=post-header><header><h1 class="p-name post-title"><a class=u-url href=../../blog/neuroevo-lora/>Low-Rank Factorizations are Indirect Encodings for Deep Neuroevolution</a></h1></header></div><div class="content post-summary p-summary"><p>My latest paper is available on arxiv: <a href=https://arxiv.org/abs/2504.03037>Low Rank Factorizations are Indirect Encodings for Deep Neuroevolution</a>.</p><p>The general idea is that we can search for stronger neural networks in a gradient-free fashion by restricting search to networks of low-rank. We show that it works well for language modeling and reinforcement learning tasks. It&rsquo;s essentially a crossover between the following papers:</p><ul><li><a href=https://arxiv.org/abs/2106.09685>LoRA: Low-Rank Adaptation of Large Language Models</a></li><li><a href=https://arxiv.org/abs/1712.06567>Deep Neuroevolution: Genetic Algorithms Are a Competitive Alternative for Training Deep Neural Networks for Reinforcement Learning</a>.</li></ul><p>I&rsquo;ll be presenting it virtually for the <a href=https://newk-gecco.github.io/>Neuroevolution@Work</a> workshop at <a href=https://gecco-2025.sigevo.org/HomePage>GECCO 2025</a>.</p></div><div class=post-info><div class="post-date dt-published">2025-05-29</div><a class="post-hidden-url u-url" href=//localhost:1313/blog/neuroevo-lora/>//localhost:1313/blog/neuroevo-lora/</a>
<a href=//localhost:1313/ class="p-name p-author post-hidden-author h-card" rel=me>jarbus</a><div class=post-taxonomies><ul class=post-tags><li><a href=//localhost:1313/tags/ai/>#Ai</a></li></ul></div></div></article><article class="post-list h-feed"><div class=post-header><header><h1 class="p-name post-title"><a class=u-url href=../../blog/from-reinforce-to-r1-an-abridged-genealogy/>From REINFORCE to R1: an Abridged Genealogy of Reinforcement Learning</a></h1></header></div><div class="content post-summary p-summary"><p>Starting from REINFORCE, the original deep reinforcement learning algorithm, we will trace the evolution of policy gradient methods to the Group Relative Policy Optimization algorithm used to train <a href=https://github.com/deepseek-ai/DeepSeek-R1>Deepseek r1</a>.</p><p>This post ignores the LLM side of things, less-related developments in RL, and most of the equations used for these algorithms, but captures the essence and intuition of the RL-timeline without wasting your time. This is all self-study, so feel free to send me any corrections/suggestions.<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup></p></div><div class=read-more><a class=u-url href=../../blog/from-reinforce-to-r1-an-abridged-genealogy/>Read more</a></div><div class=post-info><div class="post-date dt-published">2025-02-21</div><a class="post-hidden-url u-url" href=//localhost:1313/blog/from-reinforce-to-r1-an-abridged-genealogy/>//localhost:1313/blog/from-reinforce-to-r1-an-abridged-genealogy/</a>
<a href=//localhost:1313/ class="p-name p-author post-hidden-author h-card" rel=me>jarbus</a><div class=post-taxonomies><ul class=post-tags><li><a href=//localhost:1313/tags/ai/>#Ai</a></li></ul></div></div></article><article class="post-list h-feed"><div class=post-header><header><h1 class="p-name post-title"><a class=u-url href=../../blog/the-penultimate-wave-of-ai/>The Penultimate Wave of AI</a></h1></header></div><div class="content post-summary p-summary"><p>I don&rsquo;t think <a href=https://github.com/deepseek-ai/DeepSeek-R1>r1</a> will get us to artificial super intelligence, but whatever comes next probably will.</p><p>We are reaching a familiar bottleneck in AI. Previously, humans had to manually hardcode the patterns that AI could recognize. With deep learning, machines began to learn patterns on their own, without human assistance. With (relatively) expensive humans out of the loop, we threw machines at the world&rsquo;s data until they began to talk, code, and paint. Many people believed this would be sufficient to reach artificial super intelligence&ndash;but it wasn&rsquo;t.</p></div><div class=read-more><a class=u-url href=../../blog/the-penultimate-wave-of-ai/>Read more</a></div><div class=post-info><div class="post-date dt-published">2025-01-28</div><a class="post-hidden-url u-url" href=//localhost:1313/blog/the-penultimate-wave-of-ai/>//localhost:1313/blog/the-penultimate-wave-of-ai/</a>
<a href=//localhost:1313/ class="p-name p-author post-hidden-author h-card" rel=me>jarbus</a><div class=post-taxonomies><ul class=post-tags><li><a href=//localhost:1313/tags/ai/>#Ai</a></li><li><a href=//localhost:1313/tags/programming/>#Programming</a></li></ul></div></div></article><article class="post-list h-feed"><div class=post-header><header><h1 class="p-name post-title"><a class=u-url href=../../blog/upwards-pressure-on-originality/>Originality in the Age of AI</a></h1></header></div><div class="content post-summary p-summary"><p>It used to be good enough just to copy others. Now, with AI in the hands of billions, there&rsquo;s little value in copying.</p><p>For instance, take programming. Five years ago, building apps, websites, or games required a non-trivial amount of skill, and getting your first project off the ground was an accomplishment. Now, AI can generate most starter projects in hours, if not minutes. I think this decimates the reward, both internal and external, of actually completing the first few projects.</p></div><div class=read-more><a class=u-url href=../../blog/upwards-pressure-on-originality/>Read more</a></div><div class=post-info><div class="post-date dt-published">2024-10-05</div><a class="post-hidden-url u-url" href=//localhost:1313/blog/upwards-pressure-on-originality/>//localhost:1313/blog/upwards-pressure-on-originality/</a>
<a href=//localhost:1313/ class="p-name p-author post-hidden-author h-card" rel=me>jarbus</a><div class=post-taxonomies><ul class=post-tags><li><a href=//localhost:1313/tags/ai/>#Ai</a></li><li><a href=//localhost:1313/tags/programming/>#Programming</a></li></ul></div></div></article><article class="post-list h-feed"><div class=post-header><header><h1 class="p-name post-title"><a class=u-url href=../../blog/emergent-trade/>Emergent Trade and Tolerated Theft Using Multi-Agent Reinforcement Learning</a></h1></header></div><div class="content post-summary p-summary"><p>I&rsquo;ve been an author on a few papers before, but I recently published the first research project where I was responsible for most of the work and direction. It&rsquo;s in the first 2024 issue of the journal <em>Artificial Life</em>, which you can find <a href=https://direct.mit.edu/artl/article-abstract/doi/10.1162/artl_a_00423/119154/Emergent-Resource-Exchange-and-Tolerated-Theft>here</a>. You can find a non-paywalled version <a href=../../trade-paper.pdf>here</a> Below, I tell the chronology of the project and summarize our findings.</p><p><img src=../../trade.gif alt="Emergent Trade" loading=lazy></p><p>We explore the conditions under which trade can emerge between four deep reinforcement learning agents that pick up and put down resources in a 2D foraging environment. Agents are rewarded for having both resources once, but the resources are distributed far apart from each other. To maximize reward, agents need to split up the work - agent 1 goes to resource A, agent 2 goes to resource B, etc, and then they meet to exchange resources, since meeting halfway can get them the most of each resource in the shortest amount of time.</p></div><div class=read-more><a class=u-url href=../../blog/emergent-trade/>Read more</a></div><div class=post-info><div class="post-date dt-published">2024-02-04</div><a class="post-hidden-url u-url" href=//localhost:1313/blog/emergent-trade/>//localhost:1313/blog/emergent-trade/</a>
<a href=//localhost:1313/ class="p-name p-author post-hidden-author h-card" rel=me>jarbus</a><div class=post-taxonomies><ul class=post-tags><li><a href=//localhost:1313/tags/programming/>#Programming</a></li><li><a href=//localhost:1313/tags/ai/>#Ai</a></li></ul></div></div></article><article class="post-list h-feed"><div class=post-header><header><h1 class="p-name post-title"><a class=u-url href=../../blog/ai-index/>AI Index</a></h1></header></div><div class="content post-summary p-summary"><p>An ever-expanding list of concepts in the field of AI to give myself and others an easy reference.
Each item in the list contains a short, rudimentary definition I&rsquo;ve written, as well as a link to a resource that can explain it better.</p><p><a href=https://stats.stackexchange.com/questions/380040/what-is-an-ablation-study-and-is-there-a-systematic-way-to-perform-it>Ablation Study</a>:
Removing some parts of a machine learning model to measure impact on performance</p><p><a href=https://mc.ai/advantage-function-in-deep-reinforcement-learning/>Advantage Function</a>: The difference between a Q-value for a state-action pair and a value for the state. Useful to determine how good an action is relative to its state.</p></div><div class=read-more><a class=u-url href=../../blog/ai-index/>Read more</a></div><div class=post-info><div class="post-date dt-published">2021-03-19</div><a class="post-hidden-url u-url" href=//localhost:1313/blog/ai-index/>//localhost:1313/blog/ai-index/</a>
<a href=//localhost:1313/ class="p-name p-author post-hidden-author h-card" rel=me>jarbus</a><div class=post-taxonomies><ul class=post-tags><li><a href=//localhost:1313/tags/technology/>#Technology</a></li><li><a href=//localhost:1313/tags/ai/>#Ai</a></li></ul></div></div></article><article class="post-list h-feed"><div class=post-header><header><h1 class="p-name post-title"><a class=u-url href=../../blog/tesla-and-false-advertising-in-ai/>Tesla and False Advertising in AI</a></h1></header></div><div class="content post-summary p-summary"><p>Here&rsquo;s the problem with advertising AI-based technology that doesn&rsquo;t exist:</p><p><strong>You cannot promise anything about your product.</strong></p><p>We&rsquo;ve all seen AI advertised to the masses that doesn&rsquo;t work as advertised, just look at any voice-to-text system. When I got my Apple Watch, I hoped to use it to respond to messages without getting distracted by my phone. I quickly realized that wasn&rsquo;t a viable solution: I had to repeat my message multiple times per text in order to get the correct dictation.</p></div><div class=read-more><a class=u-url href=../../blog/tesla-and-false-advertising-in-ai/>Read more</a></div><div class=post-info><div class="post-date dt-published">2020-07-22</div><a class="post-hidden-url u-url" href=//localhost:1313/blog/tesla-and-false-advertising-in-ai/>//localhost:1313/blog/tesla-and-false-advertising-in-ai/</a>
<a href=//localhost:1313/ class="p-name p-author post-hidden-author h-card" rel=me>jarbus</a><div class=post-taxonomies><ul class=post-tags><li><a href=//localhost:1313/tags/technology/>#Technology</a></li><li><a href=//localhost:1313/tags/ai/>#Ai</a></li></ul></div></div></article></main><footer class=common-footer><div class=common-footer-bottom><script>document.addEventListener("DOMContentLoaded",function(){document.documentElement.setAttribute("data-theme","dark"),showContent()});function showContent(){document.body.style.visibility="visible",document.body.style.opacity=1}</script></div><p class="h-card vcard"><a href=//localhost:1313/ class="p-name u-url url fn" rel=me>jarbus</a>
/
<a class="p-email u-email email" rel=me href=mailto:jarbus@tutanota.com>jarbus@tutanota.com</a></p></footer></div></body></html>