<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Ai on</title><link>/tags/ai/</link><description> (Ai)</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Fri, 21 Feb 2025 19:58:43 -0500</lastBuildDate><atom:link href="/tags/ai/index.xml" rel="self" type="application/rss+xml"/><item><title>From REINFORCE to R1: an Abridged Genealogy of Reinforcement Learning</title><link>/blog/from-reinforce-to-r1-an-abridged-genealogy/</link><pubDate>Fri, 21 Feb 2025 19:58:43 -0500</pubDate><guid>/blog/from-reinforce-to-r1-an-abridged-genealogy/</guid><description>&lt;p>Starting from REINFORCE, the original deep reinforcement learning algorithm, we will trace the evolution of policy gradient methods to the Group Relative Policy Optimization algorithm used to train &lt;a href="https://github.com/deepseek-ai/DeepSeek-R1">Deepseek r1&lt;/a>.&lt;/p>
&lt;p>This post ignores the LLM side of things, less-related developments in RL, and most of the equations used for these algorithms, but captures the essence and intuition of the RL-timeline without wasting your time. This is all self-study, so feel free to send me any corrections/suggestions.&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>&lt;/p>
&lt;h1 id="reinforcement-learning-101" >Reinforcement Learning 101
&lt;span>
&lt;a href="#reinforcement-learning-101">
&lt;svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg">&lt;path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/>&lt;path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/>&lt;/svg>
&lt;/a>
&lt;/span>
&lt;/h1>&lt;p>Reinforcement learning is a subfield of machine learning that can learn without human data. Instead, an agent can learn through interacting with its environment by taking actions and receiving rewards based on its actions. The agent wants to maximize its reward.&lt;/p>
&lt;p>Specifically, an agent receives an observation, $o_t$, from the environment at time $t$, and produces probabilities to select an action, $a_t$, based on the observation. The environment transitions to a new state, $s_{t+1}$, and the agent receives a reward, $r_{t+1}$, based on the action it took. The agent&amp;rsquo;s goal is to learn a policy, $\pi(a_t|o_t)$ that maximizes the expected sum of rewards, $R = \sum_{t=0}^T \gamma^t r_t$, where $T$ is the number of steps in the trajectory (sequence of states, actions, and rewards from the start to the end of an episode), and $\gamma$ is a discount factor, which reduces the value of rewards the farther into the future they are. There are ways to perform reinforcement learning without neural networks, but we will focus on deep reinforcement learning, where all policy actions are determined by a neural network with parameters $\theta$.&lt;/p>
&lt;h1 id="reinforce" >REINFORCE
&lt;span>
&lt;a href="#reinforce">
&lt;svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg">&lt;path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/>&lt;path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/>&lt;/svg>
&lt;/a>
&lt;/span>
&lt;/h1>&lt;p>The original policy gradient, and the basis for all policy gradient methods, is the &lt;a href="https://dilithjay.com/blog/reinforce-a-quick-introduction-with-code">REINFORCE&lt;/a> algorithm. REINFORCE samples trajectories from the environment, which it uses to directly compute the gradient of the policy:&lt;/p>
$$
\theta_{t+1} = \theta_t + \alpha R_t \sum_{t=0}^T \nabla_\theta \log \pi(a_t|o_t)
$$&lt;p>where $\alpha$ is the learning rate, and $R_t$ is the remaining discounted reward&amp;ndash;also called a return&amp;ndash;from the current state given by $R_t = \sum_{t&amp;rsquo;=t}^T \gamma^{t&amp;rsquo;-t} r_{t&amp;rsquo;}$.&lt;/p>
&lt;p>REINFORCE simply &amp;ldquo;reinforces&amp;rdquo; the actions proportional to the reward they receive. This is the most intuitive reinforcement learning algorithm, but it has high variance, which leads us to actor critics.&lt;/p>
&lt;p>I struggled with the concept of variance in RL for a while, but it&amp;rsquo;s essentially the amount of randomness in the reward signal. When you are trying to learn purely from experience, rewards from a given state can vary wildly between trajectories depending on how the environment plays out, so we need a lot of samples to get a rough idea of what the &amp;ldquo;true&amp;rdquo; reward is for a given state. Actor-critic methods reduce this variance by using another neural network called a &amp;ldquo;critic&amp;rdquo; to estimate the value of a state. It turns out that using a consistent estimator, like a neural network, stabilizes learning, as the learning signal doesn&amp;rsquo;t fluctuate as much between samples.&lt;/p>
&lt;h1 id="from-reinforce-to-actor-critic" >From REINFORCE to Actor-Critic
&lt;span>
&lt;a href="#from-reinforce-to-actor-critic">
&lt;svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg">&lt;path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/>&lt;path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/>&lt;/svg>
&lt;/a>
&lt;/span>
&lt;/h1>&lt;p>As mentioned, actor-critic methods use neural networks to estimate reward. This results in more stable learning compared to using trajectories directly. The actor is just the policy from REINFORCE, and the critic is a new, separate network that estimates the return given the current policy. We won&amp;rsquo;t cover the training of the critic here, but just assume it&amp;rsquo;s just a neural network that takes observations $o_i$ as input and estimates the return $R$ for the actor.&lt;/p>
&lt;p>At it&amp;rsquo;s core, actor-critic is just REINFORCE, but $R_t$ is computed by another neural network, which is trained using the actual returns from the environment.&lt;/p>
&lt;p>As a bonus, we can now update actors without requiring the full trajectory, since we are using an estimate instead of the true return.&lt;/p>
&lt;h1 id="from-actor-critic-to-advantage-actor-critic-a2c" >From Actor-Critic to Advantage Actor-Critic (A2C)
&lt;span>
&lt;a href="#from-actor-critic-to-advantage-actor-critic-a2c">
&lt;svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg">&lt;path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/>&lt;path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/>&lt;/svg>
&lt;/a>
&lt;/span>
&lt;/h1>&lt;p>The next step in the evolution of policy gradient methods is the Advantage Actor-Critic (A2C) algorithm. Now we introduce the &lt;em>advantage function&lt;/em>, which is the difference between the return and the value estimate of the state:&lt;/p>
$$
A_t = r + \gamma V(o_{t+1}) - V(o_t)
$$&lt;p>where $V(o_i)$ is the value estimate of the state computed by our critic at time $i$. The advantage function tells us how much better the action was than the average action taken in that state, which allows us to reinforce actions based on how much they improve our expected reward. Conversely, this also discourages actions based on how much they decrease our expected reward. This is unlike prior methods, which might only positively (or negatively) reinforce actions based on the absolute reward they receive, not the relative reward. Without the advantage function, given an environment which only produces positive rewards, we might only reinforce good actions, without ever penalizing bad ones.&lt;/p>
&lt;p>Now, the policy gradient is (roughly) computed as:&lt;/p>
$$
\theta_{t+1} = \theta_t + \alpha A_t \sum_{t=0}^T \nabla_\theta \log \pi(a_t|o_t)
$$&lt;h1 id="from-advantage-actor-critic-to-proximal-policy-optimization-ppo" >From Advantage Actor-Critic to Proximal Policy Optimization (PPO)
&lt;span>
&lt;a href="#from-advantage-actor-critic-to-proximal-policy-optimization-ppo">
&lt;svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg">&lt;path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/>&lt;path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/>&lt;/svg>
&lt;/a>
&lt;/span>
&lt;/h1>&lt;p>Skipping over many other developments, we jump directly from A2C to Proximal Policy Optimization (PPO). PPO uses the advantage function to update the policy, but just by a little bit, by introducing a &lt;em>clipping&lt;/em> term that limits the size of the update. This prevents the policy from changing its behavior too much in a given update, which would otherwise destabilize training and cause the policy to forget some learned behaviors.&lt;/p>
&lt;p>PPO prevents behaviors from changing too much by ensuring that the probability of taking an action in a given state doesn&amp;rsquo;t change too much, using a ratio of the new policy to the old policy, $\frac{\pi_\theta(a_t|o_t)}{\pi_{\theta_{\text{old}}}(a_t|o_t)}$. When the probabilities don&amp;rsquo;t change, $ \frac{\pi_\theta(a_t|o_t)}{\pi_{\theta_{\text{old}}}(a_t|o_t)}= \mathbf{1}$, but if the probabilities change, $\frac{\pi_\theta(a_t|o_t)}{\pi_{\theta_{\text{old}}}(a_t|o_t)}$ will range between 0 and $\infty$. PPO clips the ratio to a range of $[1-\epsilon, 1+\epsilon]$, where $\epsilon$ is a hyperparameter between 0 and 1 that determines the max percentage by which a probability can change.&lt;/p>
&lt;p>The PPO update is then:&lt;/p>
$$
\theta_{t+1} = \theta_t + \alpha \nabla_\theta \mathbb{E}_t \left[ \min( \frac{\pi_\theta(a_t|o_t)}{\pi_{\theta_{\text{old}}}(a_t|o_t)}
A_t, \text{clip}(\frac{\pi_\theta(a_t|o_t)}{\pi_{\theta_{\text{old}}}(a_t|o_t)} , 1-\epsilon, 1+\epsilon) A_t) \right]
$$&lt;p>This adds a lot of terms to our update, but hopefully the jump from A2C to PPO is still visible.&lt;/p>
&lt;h1 id="from-proximal-policy-optimization-to-group-relative-policy-optimization" >From Proximal Policy Optimization to Group Relative Policy Optimization
&lt;span>
&lt;a href="#from-proximal-policy-optimization-to-group-relative-policy-optimization">
&lt;svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg">&lt;path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/>&lt;path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/>&lt;/svg>
&lt;/a>
&lt;/span>
&lt;/h1>&lt;p>Finally, we arrive at Group Relative Policy Optimization (GRPO), the algorithm used to train Deepseek r1. This algorithm is used strictly in language modeling, so the ability to update before the end of a sequence doesn&amp;rsquo;t really matter; all that matters is the stability and efficiency.&lt;/p>
&lt;p>Turns out, for language modeling, we don&amp;rsquo;t actually need the critic network, which saves us 50% of the memory and computation required to run reinforcement learning. We can just take all the other non-critic enhancements since REINFORCE and apply them in large batches , which is what GRPO does.&lt;/p>
$$
\frac{1}{G}\sum_{i=1}^G \frac{1}{|o_i|} \sum_{t=1}^{|o_i|} \min \left[ \frac{\pi_\theta(o_{i,t}|q, o_{i,\lt t})}{\pi_{\theta_{\text{old}}}(o_{i,t}|q, o_{i,\lt t})} \hat{A}_{i,t}, \text{clip}\left(\frac{\pi_\theta(o_{i,t}|q, o_{i,\lt t})}{\pi_{\theta_{\text{old}}}(o_{i,t}|q, o_{i,\lt t})}, 1-\epsilon, 1+\epsilon\right) \hat{A}_{i,t} \right] - \beta D_{\text{KL}}\left[\pi_\theta \, \| \, \pi_{\text{ref}}
\right]
$$&lt;p>You can see the lineage from PPO here. $q$ refers to a question, and $o$ here refers to an output sequence. $G$ is just the number of outputs in a a group. The main difference from PPO is that, instead of computing the advantage $A_t$ using a critic network, we now compute it within a group of sequences:&lt;/p>
$$
\hat{A}_{i,t} = \frac{r_{i,t} - \text{mean}(\mathbf{r})}{\text{std}(\mathbf{r})}
$$&lt;p>where $\mathbf{r}$ is the set of returns from the group of sequences. This replaces our neural estimate with an empirical estimate, while maintaining the benefits of the critic&amp;rsquo;s stability.&lt;/p>
&lt;p>There&amp;rsquo;s a KL-divergence term at the end $\beta D_{\text{KL}}\left[\pi_\theta , | , \pi_{\text{ref}}\right]$, which is ensures that the policy doesn&amp;rsquo;t change too much from the fine-tuned policy, $\pi_{\text{ref}}$.&lt;/p>
&lt;div class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1">
&lt;p>I know I&amp;rsquo;m pretty loose with the notations&amp;ndash;I prefer this to adding a ton of extra symbols that don&amp;rsquo;t contribute much.&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title>The Penultimate Wave of AI</title><link>/blog/the-penultimate-wave-of-ai/</link><pubDate>Tue, 28 Jan 2025 19:46:23 -0500</pubDate><guid>/blog/the-penultimate-wave-of-ai/</guid><description>&lt;p>I don&amp;rsquo;t think &lt;a href="https://github.com/deepseek-ai/DeepSeek-R1">r1&lt;/a> will get us to artificial super intelligence, but whatever comes next probably will.&lt;/p>
&lt;p>We are reaching a familiar bottleneck in AI. Previously, humans had to manually hardcode the patterns that AI could recognize. With deep learning, machines began to learn patterns on their own, without human assistance. With (relatively) expensive humans out of the loop, we threw machines at the world&amp;rsquo;s data until they began to talk, code, and paint. Many people believed this would be sufficient to reach artificial super intelligence&amp;ndash;but it wasn&amp;rsquo;t.&lt;/p>
&lt;p>We ran out of data. Luckily, our newborn bots could talk 24/7&amp;ndash;not just to hundreds of millions of people, but also to other programs. These programs would ask the bots questions, verify their answer, and then use the correct answers to further improve the models. Free, infinite data. If the world&amp;rsquo;s data wasn&amp;rsquo;t enough, then infinite data must be&amp;ndash;but it wasn&amp;rsquo;t.&lt;/p>
&lt;p>We are now watching the rise of reasoners&amp;ndash;models that &amp;ldquo;think&amp;rdquo; before giving an answer. Models which generate a series of words to raise the probability of eventually producing something we want. Surely, &lt;em>surely&lt;/em>, this is it. Once we train a model to reason, it will be able to reason about its own answers, and somehow, magically, self-improve.&lt;/p>
&lt;p>This infinite self-improvement probably won&amp;rsquo;t happen. In the same way that a fixed amount of mass can&amp;rsquo;t produce infinite energy, I suspect a fixed amount of information can&amp;rsquo;t produce infinite intelligence, no matter how much we feed it back into itself. Fundamentally, a model needs information from the outside, whether that information is a response from an external system or a human filtering its prior output for quality.&lt;/p>
&lt;p>We provide some external information using formal verification systems, which is why math and programming performace is the &lt;a href="https://openai.com/index/learning-to-reason-with-llms/">prominent flex&lt;/a> from the latest models. But most domains, like biology, business, and rocket science, are not so lucky; the correct answer is rarely obvious, even for programming. How does one automatically verify an interface is easy to use?&lt;/p>
&lt;p>The answer still comes down to humans. Much to the dismay of a few arrogant men in San Francisco, people are still a key ingredient. This time, we don&amp;rsquo;t merely tell machines what patterns to look for; rather, we filter synthetic data to &amp;ldquo;steer&amp;rdquo; the model towards answers we prefer, i.e, we tell them what patterns to look for, but cheaper.&lt;/p>
&lt;p>At this point, if the right mixture of curated data could yield superintelligence, it would have&amp;ndash;models are now &lt;a href="https://arxiv.org/abs/2501.12948v1">crushing benchmarks which PhDs struggle on&lt;/a>. This is ridiculous. We&amp;rsquo;re going to wring these models of performance until they&amp;rsquo;re dry, and even then they won&amp;rsquo;t be superintelligent. Aside from extending formal verification, the way forward still seems to be curating the reasoning data these models generate and feeding it back in. As chains of reasoning become longer and more complex, human-curated data will likely yield increasingly diminishing returns.&lt;/p>
&lt;p>We&amp;rsquo;ll be approaching the limit of what these systems can do. Don&amp;rsquo;t get me wrong, they can do a &lt;em>lot&lt;/em>&amp;ndash;they&amp;rsquo;re basically magic&amp;ndash;but they can&amp;rsquo;t generally self-improve without humans or verifiers. And honestly, I can&amp;rsquo;t imagine what else comes next besides self-improving AI. We don&amp;rsquo;t know what that looks like&amp;ndash;maybe some &lt;a href="https://puffer.ai/">multiagent game-playing system&lt;/a>&amp;ndash;but whatever it is, it will be the last. For real, this time. Probably.&lt;/p></description></item><item><title>Originality in the Age of AI</title><link>/blog/upwards-pressure-on-originality/</link><pubDate>Sat, 05 Oct 2024 09:23:29 -0700</pubDate><guid>/blog/upwards-pressure-on-originality/</guid><description>&lt;p>It used to be good enough just to copy others. Now, with AI in the hands of billions, there&amp;rsquo;s little value in copying.&lt;/p>
&lt;p>For instance, take programming. Five years ago, building apps, websites, or games required a non-trivial amount of skill, and getting your first project off the ground was an accomplishment. Now, AI can generate most starter projects in hours, if not minutes. I think this decimates the reward, both internal and external, of actually completing the first few projects.&lt;/p>
&lt;p>First, it&amp;rsquo;s less impressive, which, in my opinion, matters. We like the feeling of pride we get when we show our work to others, especially when just starting out. It&amp;rsquo;s sad to diminish the relative value of someone&amp;rsquo;s work when they are just starting.&lt;/p>
&lt;p>Second, it&amp;rsquo;s less satisfying for us. For those who just want to build useful tools, modern technology is great. But, for those who want to feel a sense of reward for creating something, it&amp;rsquo;s sad, I think. There&amp;rsquo;s still lots to build, but for most projects, especially starter projects, the internal payoff&amp;rsquo;s been greatly diminished.&lt;/p>
&lt;p>This might have long-term consequences, because to get to the point where you can build something truly original, you first have to build many things which are unoriginal. The end goal of making something new is still just as valuable, if not more so, but the journey to develop the skills and original voice seems more mundane than ever. I also suspect that the next generation, who learns from AI will be significantly less technically competent, as they may never need to understand the technology they are working with.&lt;/p>
&lt;p>I think this will place an upwards pressure on originality and novelty. Now that both the technical barrier to entry and the cost of producing unoriginal work is so low, society will start to value original ideas more than ever&amp;ndash;doubly so if we reduce the rewards of the journey towards becoming original and skilled. If the world becomes flooded with less original, less technical users of AI, the value of technical competence and originality will skyrocket.&lt;/p></description></item><item><title>Emergent Trade and Tolerated Theft Using Multi-Agent Reinforcement Learning</title><link>/blog/emergent-trade/</link><pubDate>Sun, 04 Feb 2024 12:14:25 -0500</pubDate><guid>/blog/emergent-trade/</guid><description>&lt;p>I&amp;rsquo;ve been an author on a few papers before, but I recently published the first research project where I was responsible for most of the work and direction. It&amp;rsquo;s in the first 2024 issue of the journal &lt;em>Artificial Life&lt;/em>, which you can find &lt;a href="https://direct.mit.edu/artl/article-abstract/doi/10.1162/artl_a_00423/119154/Emergent-Resource-Exchange-and-Tolerated-Theft">here&lt;/a>. You can find a non-paywalled version &lt;a href="../../trade-paper.pdf">here&lt;/a> Below, I tell the chronology of the project and summarize our findings.&lt;/p>
&lt;p>&lt;img src="../../trade.gif" alt="Emergent Trade" loading="lazy">
&lt;/p>
&lt;p>We explore the conditions under which trade can emerge between four deep reinforcement learning agents that pick up and put down resources in a 2D foraging environment. Agents are rewarded for having both resources once, but the resources are distributed far apart from each other. To maximize reward, agents need to split up the work - agent 1 goes to resource A, agent 2 goes to resource B, etc, and then they meet to exchange resources, since meeting halfway can get them the most of each resource in the shortest amount of time.&lt;/p>
&lt;p>I was working on this for a while in isolation, getting nowhere, and slowly going crazy. A few months later, Deepmind published a &lt;a href="https://arxiv.org/abs/2205.06760">paper&lt;/a> that explored this exact behavior&amp;mdash;and found that agents could not discover how to exchange goods without programming a trading system into the game mechanics directly. I was getting similar results, but found that this trading behavior could emerge if I rewarded agents for being physically close to one another, which we&amp;rsquo;ll call a &amp;ldquo;community bonus&amp;rdquo;.&lt;/p>
&lt;p>I reached out to the authors (great people btw), and they hypothesized that the trading behavior was a result of agents trying to keep each other alive to get the community bonus. In my setup, agents could die if they ran out of resources, so they were also keeping each other alive for company. The authors suggested I add some sort of &amp;ldquo;market&amp;rdquo;, a place for agents to meet up and trade without getting directly rewarded for giving away resources for free.&lt;/p>
&lt;p>They explored similar ideas in their work, and if they couldn&amp;rsquo;t get this behavior to emerge, what chance did I have? I needed to try something similar but sufficiently different.&lt;/p>
&lt;p>Instead of adding a market where agents could come and go as they please, I added a campfire and a day-night cycle. I gave negative reward to agents for being out in the dark, which incentivizes them to gather around the campfire near others. Agents would only have enough time during the day to forage a single resource.&lt;/p>
&lt;p>As it turns out, agents really, &lt;em>really&lt;/em> don&amp;rsquo;t want to learn this behavior. It&amp;rsquo;s easy for agents to get cheated out of the resources they offer by others who offer nothing in return, so they gather as much as possible on their own. They even forage at night if the darkness penalty isn&amp;rsquo;t severe enough.&lt;/p>
&lt;p>Eventually, after 4+ days of training time, agents start to realize that working together isn&amp;rsquo;t so bad and land on a trading protocol where agents stand back, drop an offer, then walk over to collect the resources dropped by others. Experiments show that agents keep their distance initially so they can reclaim their offer if their trading partner attempts to cheat them.&lt;/p>
&lt;p>In one trial, agents converge on a local minimum, where agents 1&amp;amp;2 go to the same area to collect resource A, agent 3 goes to collect resource B, and agent 4 collects a bit of the other remaining resources. In this setting, we notice a behavior which we dub &amp;ldquo;Tolerated Theft&amp;rdquo;. When agent 1 collects significantly more of resource A than agent 2, agent 2 tries to steal the resources exchanged between agents 1 and 3. To defend against this, agent 1 will drop some resources away from agent 3, to get agent 2 off their backs while they trade. This is particularly interesting, because an agent manages to &amp;ldquo;steal&amp;rdquo; resources without us adding a combat or larceny system. Nature truly does find a way.&lt;/p>
&lt;p>I recommend checking out the &lt;a href="https://arxiv.org/abs/2307.01862">arxiv pre-print&lt;/a> if you are interested in the details and figures.&lt;/p></description></item><item><title>AI Index</title><link>/blog/ai-index/</link><pubDate>Fri, 19 Mar 2021 11:25:49 -0400</pubDate><guid>/blog/ai-index/</guid><description>&lt;p>An ever-expanding list of concepts in the field of AI to give myself and others an easy reference.
Each item in the list contains a short, rudimentary definition I&amp;rsquo;ve written, as well as a link to a resource that can explain it better.&lt;/p>
&lt;p>&lt;a href="https://stats.stackexchange.com/questions/380040/what-is-an-ablation-study-and-is-there-a-systematic-way-to-perform-it">Ablation Study&lt;/a>:
Removing some parts of a machine learning model to measure impact on performance&lt;/p>
&lt;p>&lt;a href="https://mc.ai/advantage-function-in-deep-reinforcement-learning/">Advantage Function&lt;/a>: The difference between a Q-value for a state-action pair and a value for the state. Useful to determine how good an action is relative to its state.&lt;/p>
&lt;p>&lt;a href="https://towardsdatascience.com/intuitive-understanding-of-attention-mechanism-in-deep-learning-6c9482aecf4f">Attention&lt;/a>: Neural networks are able to &amp;ldquo;pay attention&amp;rdquo; to specific parts of input or output, useful in translating languages or predicting sequences. For example, when trying to predict the next word in &amp;ldquo;the clouds in the&amp;rdquo;, you pay attention to the word &lt;em>cloud&lt;/em> to predict the word &lt;em>sky&lt;/em>&lt;/p>
&lt;p>&lt;a href="https://en.wikipedia.org/wiki/Autoencoder">Autoencoder&lt;/a>&lt;sup>W&lt;/sup>: A type of neural network that attempts to take raw data, convert it into a simpler representation (usually by limiting the amount of nodes in a hidden layer for representation), and then decode the representation back into it&amp;rsquo;s data. They are primarly used to extract the useful properties from data automatically. They can do this in an unsupervised fashion, since their output targets are the given input, requiring no labeling.&lt;/p>
&lt;p>&lt;a href="https://en.wikipedia.org/wiki/Automated_machine_learning">AutoML&lt;/a>&lt;sup>W&lt;/sup>: Systems where the entire machine learning process, from data preparation to network design, is automated.&lt;/p>
&lt;p>&lt;a href="https://machinelearningmastery.com/autoregression-models-time-series-forecasting-python/">Autoregression&lt;/a>: A time series model that uses observations from previous time steps to predict the value at the next time step.&lt;/p>
&lt;p>&lt;a href="https://en.wikipedia.org/wiki/Backpropagation">Backpropogation&lt;/a>&lt;sup>W&lt;/sup>: The algorithm and calculus behind gradient descent traditionally used in feed-forward neural networks&lt;/p>
&lt;p>&lt;a href="https://en.wikipedia.org/wiki/Bayesian_statistics">Bayesian&lt;/a>: An interpretation of probability where the phrase &amp;ldquo;probability&amp;rdquo; expresses a degree of belief between 0 or 1, 0 representing false and 1 representing true. Uses expected values and random variables.&lt;/p>
&lt;p>&lt;a href="https://en.wikipedia.org/wiki/Bootstrapping_(statistics)">Bootstrapping&lt;/a>&lt;sup>W&lt;/sup>: Sampling from a distribution to approximate a function. In cases of reinforcement learning, bootstrapping usually samples potential future values to approximate a current value.&lt;/p>
&lt;p>&lt;a href="https://en.wikipedia.org/wiki/Convolutional_neural_network">Convolutional Neural Network (CNN)&lt;/a>&lt;sup>W&lt;/sup>: A neural network primarly used for image processing. These networks design filters for specific parts of an image to extract higher level information, filters such as detecting a certain type of edge.&lt;/p>
&lt;p>&lt;a href="https://www.quora.com/What-is-Covariate-shift?share=1">Covariate Shift&lt;/a>: The training distribution changes but the testing distribution does not change&lt;/p>
&lt;p>&lt;a href="https://en.wikipedia.org/wiki/Data_mining">Data Mining&lt;/a>: Discovering knowledge and patterns from massive amounts of data, usually in an unsupervised fashion.&lt;/p>
&lt;p>&lt;a href="https://en.wikipedia.org/wiki/Deep_learning">Deep Learning&lt;/a>&lt;sup>W&lt;/sup>: A subset of machine learning with a multi-step learning process, usually referring to neural networks with two or more layers.&lt;/p>
&lt;p>&lt;a href="https://stats.stackexchange.com/questions/221402/understanding-the-role-of-the-discount-factor-in-reinforcement-learning">Discount Factors&lt;/a>: A variable (usually $\gamma$) that determines how much a model cares about rewards in the distant future compared to immediate rewards.&lt;/p>
&lt;ul>
&lt;li>$\gamma = 0$ cares only about immediate reward,&lt;/li>
&lt;li>$\gamma = 1$ cares only about sum of all future rewards.&lt;/li>
&lt;/ul>
&lt;p>&lt;a href="https://towardsdatascience.com/eligibility-traces-in-reinforcement-learning-a6b458c019d6">Eligibility Trace&lt;/a>: For temporal learning, the eligibility trace is a vector of decaying values that represent when weights were last used. When we encounter an error, this vector allows us to update recent weights harder than weights used long ago.&lt;/p>
&lt;p>&lt;a href="https://en.wikipedia.org/wiki/Feedforward_neural_network">Feed-forward Neural Network&lt;/a>&lt;sup>W&lt;/sup>: A simple neural network where information is passed strictly from one layer to the next.&lt;/p>
&lt;p>&lt;a href="https://en.wikipedia.org/wiki/Generative_adversarial_network">Generative Adversarial Network (GAN)&lt;/a>&lt;sup>W&lt;/sup>: A set of two or more neural networks that can generate new data based on existing training data.
A simple example is &lt;a href="https://thispersondoesnotexist.com/">https://thispersondoesnotexist.com/&lt;/a>, that can generate fake pictures of humans.
- GANs consist of one generator network with the goal to make realistic new data, and a distinction network with the goal to tell real data from fake. As the networks train, they each improve at their individual task, forcing their adversaries to improve in turn.&lt;/p>
&lt;p>&lt;a href="https://en.wikipedia.org/wiki/Genetic_algorithm">Genetic Algorithms&lt;/a>&lt;sup>W&lt;/sup>: Algorithms that try to mimic the evolutionary process by randomly modifying the best-performing sets of parameters while discarding those with the worst performance, then repeating.&lt;/p>
&lt;p>&lt;a href="https://openai.com/blog/better-language-models/">GPT-2/3&lt;/a>: A language model from OpenAI that can generate text that mimics a writing style incredibly well.&lt;/p>
&lt;p>&lt;a href="https://en.wikipedia.org/wiki/Gradient_descent">Gradient Descent&lt;/a>&lt;sup>W&lt;/sup>: An interative process that attempts to find a minimum of a function that works by moving in the direction that will decrease the gradient until a local minima is reached.&lt;/p>
&lt;p>&lt;a href="https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning)">Hyperparameters&lt;/a>&lt;sup>W&lt;/sup>: Manually defined parameters of the model, such as the size of a neural network, or manually defined parameters of the machine learning algorithm, such as learning rate.&lt;/p>
&lt;p>&lt;a href="https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables">(IID) Independent and Identically Distributed&lt;/a>&lt;sup>W&lt;/sup>: Random variables are independent and identically distributed when the value of one variable doesn&amp;rsquo;t affect the probability of another variable. For example, the outcome of one coin flip does not affect the outcome of another coin flip, so both flips are IID.&lt;/p>
&lt;p>&lt;a href="https://dibyaghosh.com/blog/probability/kldivergence.html">KL Divergence&lt;/a>: Divergence between two distributions of data. Useful for determining how different fake data is from real data (GANs) or for determining how differemt two policies are for trust-based reinforcement learning.&lt;/p>
&lt;p>&lt;a href="https://en.wikipedia.org/wiki/Self-driving_car">Level 0-5&lt;/a>&lt;sup>W&lt;/sup>: The different &amp;ldquo;levels&amp;rdquo; of autonomy related to self driving cars. 0 represents full human control while 5 represents full vehicle control.&lt;/p>
&lt;p>&lt;a href="https://en.wikipedia.org/wiki/Long_short-term_memory">Long Short Term Memory (LSTM)&lt;/a>&lt;sup>W&lt;/sup>: A type of recurrent neural network that works exceptionally well with sequential input. A unique trait of these networks are their memory cells&amp;rsquo; &amp;ldquo;forget gates&amp;rdquo;, which allow them to control how long they hold onto data for.&lt;/p>
&lt;p>&lt;a href="https://deepai.org/publication/the-lottery-ticket-hypothesis-training-pruned-neural-networks">The Lottery Ticket Hypothesis&lt;/a>: A randomly-initialized, dense neural network contains a subnetwork that is initialised such that — when trained in isolation — it can match the test accuracy of the original network after training for at most the same number of iterations.&lt;/p>
&lt;p>&lt;a href="https://en.wikipedia.org/wiki/Markov_decision_process">Markov Decision Process&lt;/a>&lt;sup>W&lt;/sup>: A system of states, actions, and probabilities of getting to other states given actions taken from previous states.&lt;/p>
&lt;p>&lt;a href="https://aiden.nibali.org/blog/2017-01-18-mode-collapse-gans/">Mode Collapse&lt;/a>: When the distribution of samples produced by a generative adversarial network represent a subset of the latent distribution, instead of the entire latent distribution. For example, you train a network to produce pictures of animals but it only learns to produce pictures of cats.&lt;/p>
&lt;p>&lt;a href="https://en.wikipedia.org/wiki/Multi-armed_bandit">Multi-armed Bandit&lt;/a>&lt;sup>W&lt;/sup>: A core component of reinforcement learning, the multi-armed bandit problem is the classic &amp;ldquo;exploration versus exploitation&amp;rdquo; tradeoff. In this problem, expected gain must be maximized in an environment with varying rewards, forcing an agent to decide between keeping an option they know to be safe versus exploring new options that might be better.&lt;/p>
&lt;p>&lt;a href="https://stats.stackexchange.com/questions/897/online-vs-offline-learning">Online/Offline Learning&lt;/a>: Online learning happens as data comes in. Offline learning happens after data is collected and made into a batch.&lt;/p>
&lt;p>&lt;a href="http://mlss.tuebingen.mpg.de/2015/slides/ghahramani/gp-neural-nets15.pdf">Parametric &amp;amp; Non-Parametric Models&lt;/a>: Parametric models use a finite number of parameters, like weights in linear regression, to represent a learned hypothesis. Non-Parametric models use variable/infinite/no parameters, like data points in nearest neighbors, to represent a learned hypothesis.&lt;/p>
&lt;p>&lt;a href="https://en.wikipedia.org/wiki/Precision_and_recall">Precision&lt;/a>&lt;sup>W&lt;/sup>: The fraction of relevant instances among the retrieved instances. Also known as positive predictive value.&lt;/p>
&lt;p>&lt;a href="https://en.wikipedia.org/wiki/Precision_and_recall">Recall&lt;/a>&lt;sup>W&lt;/sup>: The fraction of relevant instances that were retrieved. Also known as sensitivity.&lt;/p>
&lt;p>&lt;a href="https://en.wikipedia.org/wiki/Recurrent_neural_network">Recurrent Neural Network (RNN)&lt;/a>&lt;sup>W&lt;/sup>: A type of network that can store state, giving it a type of memory that can process a series of inputs. This can be accomplished by having a cycle within the network.&lt;/p>
&lt;p>&lt;a href="https://en.wikipedia.org/wiki/Regression_analysis">Regression&lt;/a>&lt;sup>W&lt;/sup>: A set of models that determine relationships between data and a dependent value. For example, linear regression tries to approximate a dependent value while logistic regression tries to determine the probability of a dependent value.&lt;/p>
&lt;p>&lt;a href="https://en.wikipedia.org/wiki/Residual_neural_network">Residual Neural Network&lt;/a>&lt;sup>W&lt;/sup>: Networks with connections that skip some layers and connect to non-adjacent ones. This type of network helps counter the vanishing gradient problem&lt;/p>
&lt;p>&lt;a href="https://en.wikipedia.org/wiki/Reinforcement_learning">Reinforcement Learning&lt;/a>&lt;sup>W&lt;/sup>: Algorithms are given a world state they are able to interact with, and learn from the reward their interactions give them.&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://ai.stackexchange.com/questions/4456/whats-the-difference-between-model-free-and-model-based-reinforcement-learning">Model-Based&lt;/a>: You create a model of the world and can predict what the next state and reward will be for each action&lt;/li>
&lt;li>&lt;a href="https://ai.stackexchange.com/questions/4456/whats-the-difference-between-model-free-and-model-based-reinforcement-learning">Model-Free&lt;/a>: You know what action to take without knowing what to expect, since you don&amp;rsquo;t have a model of the world&lt;/li>
&lt;li>&lt;a href="https://www.freecodecamp.org/news/an-intro-to-advantage-actor-critic-methods-lets-play-sonic-the-hedgehog-86d6240171d/">Value&lt;/a>: Networks that determine the value of a state.&lt;/li>
&lt;li>&lt;a href="https://www.freecodecamp.org/news/an-intro-to-advantage-actor-critic-methods-lets-play-sonic-the-hedgehog-86d6240171d/">Policy&lt;/a>: Network to choose actions. Can directly optimize model instead of computing values, useful when you have a continuous action space. Only uses reward function. Requires a score function to evaluate performance of policy, usually total rewards accumulated given a period of time.&lt;/li>
&lt;li>&lt;a href="https://www.freecodecamp.org/news/an-intro-to-advantage-actor-critic-methods-lets-play-sonic-the-hedgehog-86d6240171d/">Actor Critic&lt;/a>: Backbone of state of the art reinforcement learning algorithms.
&lt;ul>
&lt;li>Uses a value-based Critic to measure how good the action taken is&lt;/li>
&lt;li>Uses a policy-based Actor to to choose actions&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;a href="https://en.wikipedia.org/wiki/Stochastic">Stochastic&lt;/a>&lt;sup>W&lt;/sup>: Randomly determined process, usually refers to probabilistic outputs of machine learning systems&lt;/p>
&lt;p>&lt;a href="https://en.wikipedia.org/wiki/Supervised_learning">Supervised Learning&lt;/a>&lt;sup>W&lt;/sup>: A model learns to produce a desired output and knows what that output is. Example: Image Recognition&lt;/p>
&lt;p>&lt;a href="https://en.wikipedia.org/wiki/Temporal_difference_learning">Temporal Difference&lt;/a>&lt;sup>W&lt;/sup>: Model-free reinforcement learning design which learns by bootstrapping value samples in order to approximate a value function. Once more information is revealed about the true value during later timesteps, you can update the low information bootstrapped guess by using the newly acquired outcome as a &amp;ldquo;ground truth&amp;rdquo; to train a network.&lt;/p>
&lt;ul>
&lt;li>Temporal Difference error is the difference between consecutive temporal predictions.&lt;/li>
&lt;/ul>
&lt;p>&lt;a href="https://ai.stackexchange.com/questions/7359/what-is-a-trajectory-in-reinforcement-learning">Trajectories&lt;/a>: The history of states (and potentially actions) taken during a walk of a Markov Decision Process.&lt;/p>
&lt;p>&lt;a href="https://en.wikipedia.org/wiki/Transfer_learning">Transfer Learning&lt;/a>&lt;sup>W&lt;/sup>: Taking parts of a network trained on one data set, and using it in a different network with a different dataset.&lt;/p>
&lt;p>&lt;a href="https://towardsdatascience.com/transformers-141e32e69591">Transformer&lt;/a>: A type of recurrent neural network primarily used with sequential data, like language translation. These networks use an attention model to improve performance.&lt;/p>
&lt;p>&lt;a href="https://en.wikipedia.org/wiki/Unsupervised_learning">Unsupervised Learning&lt;/a>&lt;sup>W&lt;/sup>: A model learns to produce a desired output without being told what it&amp;rsquo;s looking for. Example: Playing Chess&lt;/p>
&lt;p>&lt;a href="https://en.wikipedia.org/wiki/Vanishing_gradient_problem">Vanishing Gradient Problem&lt;/a>&lt;sup>W&lt;/sup>: In a network, a gradient can become vanishingly small which will stop the weight from changing it&amp;rsquo;s value, since weights are modified based on their contribution to the gradient.&lt;/p>
&lt;p>Feel free to contact me with any suggested additions/changes at &lt;a href="mailto:jarbus@tutanota.com">jarbus@tutanota.com&lt;/a>.&lt;/p></description></item><item><title>Tesla and False Advertising in AI</title><link>/blog/tesla-and-false-advertising-in-ai/</link><pubDate>Wed, 22 Jul 2020 11:25:49 -0400</pubDate><guid>/blog/tesla-and-false-advertising-in-ai/</guid><description>&lt;p>Here&amp;rsquo;s the problem with advertising AI-based technology that doesn&amp;rsquo;t exist:&lt;/p>
&lt;p>&lt;strong>You cannot promise anything about your product.&lt;/strong>&lt;/p>
&lt;p>We&amp;rsquo;ve all seen AI advertised to the masses that doesn&amp;rsquo;t work as advertised, just look at any voice-to-text system. When I got my Apple Watch, I hoped to use it to respond to messages without getting distracted by my phone. I quickly realized that wasn&amp;rsquo;t a viable solution: I had to repeat my message multiple times per text in order to get the correct dictation.&lt;/p>
&lt;p>If this was a software issue, I could file a bug report and Apple could send out a fix. Unfortunately with AI, there isn&amp;rsquo;t much Apple can do besides get more data and re-train the model, which still wouldn&amp;rsquo;t guarantee a better user experience for anyone. Mind you, this is with software that&amp;rsquo;s already shipped.&lt;/p>
&lt;p>Now let&amp;rsquo;s look at Tesla, or more specifically its CEO, Elon Musk. &lt;a href="https://futurism.com/elon-musk-tesla-close-level-5-autonomy">Musk claims that Level 5 driving (in which a human is not needed behind the wheel) will be coming to Tesla vehicles in the form of a software update by the end of 2020&lt;/a>. A month before he made said claim, &lt;a href="https://futurism.com/the-byte/tesla-autopilot-slam-overturned-truck">a Tesla on autopilot crashed into a flipped truck&lt;/a>&amp;ndash;a mistake most humans wouldn&amp;rsquo;t make.&lt;/p>
&lt;p>Just like Apple can&amp;rsquo;t promise its speech-to-text system will work better than a human (or even better than a toddler), Musk can&amp;rsquo;t guarantee anything about Tesla&amp;rsquo;s Level 5 capability. Tesla can&amp;rsquo;t even &amp;ldquo;reasonably claim&amp;rdquo; your car won&amp;rsquo;t crash into flipped trucks or &lt;a href="https://www.msn.com/en-us/autos/enthusiasts/another-tesla-crashes-into-a-cop-car-while-on-autopilot/ar-BB16Mvkk#!">stopped cop cars&lt;/a>. In fact, no one using deep learning can promise anything about their system. Deep learning, and by extension Tesla&amp;rsquo;s Autopilot system, is a black box. We don&amp;rsquo;t know why it works, we can&amp;rsquo;t promise that it&amp;rsquo;ll work, but it seems to work when we test it. Unless Tesla is testing on all roads in all conditions, they shouldn&amp;rsquo;t even be &amp;ldquo;confident&amp;rdquo; level 5 technology, in which &lt;strong>a human does not need to be in the vehicle&lt;/strong>, is &amp;ldquo;around the corner&amp;rdquo;. That&amp;rsquo;s why &lt;a href="https://www.reuters.com/article/us-tesla-autopilot-germany/german-court-bans-tesla-ad-statements-related-to-autonomous-driving-idUSKCN24F1T5">a court in Germany banned Tesla from including “full potential for autonomous driving” and “Autopilot inclusive” in its German advertising materials&lt;/a>.&lt;/p>
&lt;p>A Full Self-Driving (FSD) update wouldn&amp;rsquo;t be the first time Tesla over-promises and under-delivers. Consumer Reports described the company&amp;rsquo;s &amp;ldquo;Smart Summon&amp;rdquo; feature as &lt;a href="https://www.consumerreports.org/automotive-technology/teslas-smart-summon-performance-doesnt-match-marketing-hype/">&amp;ldquo;glitchy and at times worked intermittently&amp;rdquo;&lt;/a>. The damage with Smart Summon has been low, but a full-autonomy update could result in a few consequences:&lt;/p>
&lt;ul>
&lt;li>Tesla sold cars with the promise of Level 5 driving in the future: Not only is Tesla now under immense pressure to rush out a Level 5 update as soon as possible, but must do so using the hardware it sold customers years ago. Hopefully Level 5 doesn&amp;rsquo;t require LIDAR sensors (or any additional hardware) like Elon claims; because Tesla is expected to ship some version of FSD to users.&lt;/li>
&lt;li>Customers may end up in legal trouble if Tesla&amp;rsquo;s software fails them: Tesla avoids legal responsibility for buggy software by shifting responsibility of an accident to the driver. However, when Tesla sells millions of cars on the premise that users won&amp;rsquo;t have to manually drive them one day, should drivers be held responsible when that day comes and their car crashes?&lt;/li>
&lt;/ul>
&lt;p>Other companies have realized the high standards a Level 5 system should meet. Two years ago, &lt;a href="https://www.bloomberg.com/news/articles/2018-11-13/waymo-ceo-says-self-driving-cars-won-t-be-ubiqitious-for-decades">Waymo CEO John Krafcik claimed that true self-driving cars, that function everywhere under all circumstances, won&amp;rsquo;t be ubiquitous for decades&lt;/a>. As a result, Waymo shifted gears and placed its focus on rolling out &lt;a href="https://www.theverge.com/2019/12/9/21000085/waymo-fully-driverless-car-self-driving-ride-hail-service-phoenix-arizona">autonomous taxis&lt;/a> on a city by city basis, starting in Pheonix, Arizona. This approach to Level 5 driving is much safer for the following reasons:&lt;/p>
&lt;ul>
&lt;li>Waymo is minimizing the number of people at risk&lt;/li>
&lt;li>Waymo is operating its vehicles strictly where they have been tested.&lt;/li>
&lt;li>Waymo is not selling consumers a technology that doesn&amp;rsquo;t exist&lt;/li>
&lt;li>Waymo is not relying on the average consumer to be attentive&lt;/li>
&lt;/ul>
&lt;p>Level 5 technology can save thousands of lives and millions of dollars&amp;ndash;but it needs to be done to the same (if not higher) standards of automotive testing, and not the &amp;ldquo;move fast and break things&amp;rdquo; approach of Silicon Valley. Hopefully Tesla can safely deliver on its promise of full self driving soon, but if not, consumers may soon have a case to demand a refund.&lt;/p></description></item></channel></rss>