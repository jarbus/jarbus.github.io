<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Technology on</title><link>/tags/technology/</link><description> (Technology)</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Fri, 17 Dec 2021 14:28:41 -0500</lastBuildDate><atom:link href="/tags/technology/index.xml" rel="self" type="application/rss+xml"/><item><title>How to write LaTeX without writing LaTeX</title><link>/blog/write-latex-without-latex/</link><pubDate>Fri, 17 Dec 2021 14:28:41 -0500</pubDate><guid>/blog/write-latex-without-latex/</guid><description>&lt;p>I love the look of LaTeX but hate the experience of writing in LaTeX, at least compared to Markdown. Luckily, &lt;a href="https://pandoc.org/MANUAL.html#pandocs-Markdown">Pandoc can convert Markdown files to PDFs using a LaTeX engine as the renderer&lt;/a>, and includes a custom Markdown specification that can fill almost all my LaTeX needs.&lt;/p>
&lt;p>First, let&amp;rsquo;s talk about where Pandoc Markdown falls short:&lt;/p>
&lt;ul>
&lt;li>No custom LaTeX style guides (although citation style files are supported)&lt;/li>
&lt;li>&lt;del>No Section Numbering&lt;/del> There is, thanks to naruhodo on Hacker News for the correction&lt;/li>
&lt;li>Referencing labels doesn&amp;rsquo;t work well (Supposedly the pandoc-crossref filter fixes this but I couldn&amp;rsquo;t get it to work)&lt;/li>
&lt;li>Don&amp;rsquo;t even bother with complex page layouts or precise figure placements&lt;/li>
&lt;/ul>
&lt;p>This might be a deal breaker for some, but for others who are writing lots of documents (such as students), this may not be. Now for the benefits:&lt;/p>
&lt;ul>
&lt;li>Produce a document with Markdown that looks like it was written in LaTeX&lt;/li>
&lt;li>Use LaTeX math notation in Markdown&lt;/li>
&lt;li>Easy BibTeX citations in Markdown&lt;/li>
&lt;li>Figures and Captions are supported&lt;/li>
&lt;li>Two Columns are supported&lt;/li>
&lt;li>Use your favorite editor instead of some LaTeX IDE&lt;/li>
&lt;li>Write and preview offline&lt;/li>
&lt;li>Build a document in one command&lt;/li>
&lt;/ul>
&lt;p>If you are reading this, you probably know how both Markdown and Latex work, and you probably know how to read the &lt;a href="https://pandoc.org/MANUAL.html#pandocs-Markdown">Pandoc Markdown Documentation&lt;/a>, so instead, below is a template document that I wish I had when first starting out. It covers most things you&amp;rsquo;d need to write simple papers in Markdown. Configuration for document settings is done in the YAML block at the top of the document.&lt;/p>
&lt;pre tabindex="0">&lt;code>---
title: How to Succeed in LaTeX without even trying
subtitle: For the lazy writer
author:
- jarbus.net
numbersections: true
toc: true
geometry:
- margin=1in
linkcolor: black
urlcolor: blue
bibliography: example.bib
csl: nature.csl
header-includes: |
\usepackage{package_here}
classoption:
- twocolumn
abstract: |
This is the abstract
---
# Introduction
According to all known laws
of aviation, there is now way
that a bee should be able to fly. [@seinfeld2007]
| &amp;#34;This is a quote&amp;#34;
![Caption here](bee.png){width=50% }
$\forall b , y = mx+b$
$$
a^2 + b^2 = c^2
$$
# References
::: {#refs}
:::
# Appendix
&lt;/code>&lt;/pre>&lt;p>Then, to compile your document to a PDF, simply run:&lt;/p>
&lt;pre tabindex="0">&lt;code>pandoc &amp;#34;file.md&amp;#34; -o &amp;#34;file.pdf&amp;#34; --citeproc
&lt;/code>&lt;/pre>&lt;p>to generate a document that looks like this:&lt;/p>
&lt;p>&lt;img src="../../final-document.png" alt="" loading="lazy">
&lt;/p>
&lt;p>The &lt;code>--citeproc&lt;/code> option is needed for compiling a bibliography. For cross-references inside a document, take a look at &lt;a href="https://github.com/lierdakil/pandoc-crossref">pandoc-crossref&lt;/a>, which will require the use of &lt;code>--filter pandoc-crossref&lt;/code> if it works for you.&lt;/p>
&lt;p>I mapped a modified version of the build command above to a key in Neovim, so now pressing &lt;code>Space+b&lt;/code> in my Markdown buffer compiles my document automatically, no matter what file name I&amp;rsquo;m using:&lt;/p>
&lt;pre tabindex="0">&lt;code>autocmd BufRead,BufNewFile *.md nnoremap &amp;lt;Leader&amp;gt;b
\ :silent pandoc % -o &amp;#34;%:p:h/%:t:r.pdf&amp;#34;
\ --filter pandoc-crossref --citeproc &amp;amp;&amp;lt;CR&amp;gt;&amp;lt;CR&amp;gt;
&lt;/code>&lt;/pre>&lt;p>And that&amp;rsquo;s how I write LaTeX documents offline in Markdown.&lt;/p></description></item><item><title>AI Index</title><link>/blog/ai-index/</link><pubDate>Fri, 19 Mar 2021 11:25:49 -0400</pubDate><guid>/blog/ai-index/</guid><description>&lt;p>An ever-expanding list of concepts in the field of AI to give myself and others an easy reference.
Each item in the list contains a short, rudimentary definition I&amp;rsquo;ve written, as well as a link to a resource that can explain it better.&lt;/p>
&lt;p>&lt;a href="https://stats.stackexchange.com/questions/380040/what-is-an-ablation-study-and-is-there-a-systematic-way-to-perform-it">Ablation Study&lt;/a>:
Removing some parts of a machine learning model to measure impact on performance&lt;/p>
&lt;p>&lt;a href="https://mc.ai/advantage-function-in-deep-reinforcement-learning/">Advantage Function&lt;/a>: The difference between a Q-value for a state-action pair and a value for the state. Useful to determine how good an action is relative to its state.&lt;/p>
&lt;p>&lt;a href="https://towardsdatascience.com/intuitive-understanding-of-attention-mechanism-in-deep-learning-6c9482aecf4f">Attention&lt;/a>: Neural networks are able to &amp;ldquo;pay attention&amp;rdquo; to specific parts of input or output, useful in translating languages or predicting sequences. For example, when trying to predict the next word in &amp;ldquo;the clouds in the&amp;rdquo;, you pay attention to the word &lt;em>cloud&lt;/em> to predict the word &lt;em>sky&lt;/em>&lt;/p>
&lt;p>&lt;a href="https://en.wikipedia.org/wiki/Autoencoder">Autoencoder&lt;/a>&lt;sup>W&lt;/sup>: A type of neural network that attempts to take raw data, convert it into a simpler representation (usually by limiting the amount of nodes in a hidden layer for representation), and then decode the representation back into it&amp;rsquo;s data. They are primarly used to extract the useful properties from data automatically. They can do this in an unsupervised fashion, since their output targets are the given input, requiring no labeling.&lt;/p>
&lt;p>&lt;a href="https://en.wikipedia.org/wiki/Automated_machine_learning">AutoML&lt;/a>&lt;sup>W&lt;/sup>: Systems where the entire machine learning process, from data preparation to network design, is automated.&lt;/p>
&lt;p>&lt;a href="https://machinelearningmastery.com/autoregression-models-time-series-forecasting-python/">Autoregression&lt;/a>: A time series model that uses observations from previous time steps to predict the value at the next time step.&lt;/p>
&lt;p>&lt;a href="https://en.wikipedia.org/wiki/Backpropagation">Backpropogation&lt;/a>&lt;sup>W&lt;/sup>: The algorithm and calculus behind gradient descent traditionally used in feed-forward neural networks&lt;/p>
&lt;p>&lt;a href="https://en.wikipedia.org/wiki/Bayesian_statistics">Bayesian&lt;/a>: An interpretation of probability where the phrase &amp;ldquo;probability&amp;rdquo; expresses a degree of belief between 0 or 1, 0 representing false and 1 representing true. Uses expected values and random variables.&lt;/p>
&lt;p>&lt;a href="https://en.wikipedia.org/wiki/Bootstrapping_(statistics)">Bootstrapping&lt;/a>&lt;sup>W&lt;/sup>: Sampling from a distribution to approximate a function. In cases of reinforcement learning, bootstrapping usually samples potential future values to approximate a current value.&lt;/p>
&lt;p>&lt;a href="https://en.wikipedia.org/wiki/Convolutional_neural_network">Convolutional Neural Network (CNN)&lt;/a>&lt;sup>W&lt;/sup>: A neural network primarly used for image processing. These networks design filters for specific parts of an image to extract higher level information, filters such as detecting a certain type of edge.&lt;/p>
&lt;p>&lt;a href="https://www.quora.com/What-is-Covariate-shift?share=1">Covariate Shift&lt;/a>: The training distribution changes but the testing distribution does not change&lt;/p>
&lt;p>&lt;a href="https://en.wikipedia.org/wiki/Data_mining">Data Mining&lt;/a>: Discovering knowledge and patterns from massive amounts of data, usually in an unsupervised fashion.&lt;/p>
&lt;p>&lt;a href="https://en.wikipedia.org/wiki/Deep_learning">Deep Learning&lt;/a>&lt;sup>W&lt;/sup>: A subset of machine learning with a multi-step learning process, usually referring to neural networks with two or more layers.&lt;/p>
&lt;p>&lt;a href="https://stats.stackexchange.com/questions/221402/understanding-the-role-of-the-discount-factor-in-reinforcement-learning">Discount Factors&lt;/a>: A variable (usually $\gamma$) that determines how much a model cares about rewards in the distant future compared to immediate rewards.&lt;/p>
&lt;ul>
&lt;li>$\gamma = 0$ cares only about immediate reward,&lt;/li>
&lt;li>$\gamma = 1$ cares only about sum of all future rewards.&lt;/li>
&lt;/ul>
&lt;p>&lt;a href="https://towardsdatascience.com/eligibility-traces-in-reinforcement-learning-a6b458c019d6">Eligibility Trace&lt;/a>: For temporal learning, the eligibility trace is a vector of decaying values that represent when weights were last used. When we encounter an error, this vector allows us to update recent weights harder than weights used long ago.&lt;/p>
&lt;p>&lt;a href="https://en.wikipedia.org/wiki/Feedforward_neural_network">Feed-forward Neural Network&lt;/a>&lt;sup>W&lt;/sup>: A simple neural network where information is passed strictly from one layer to the next.&lt;/p>
&lt;p>&lt;a href="https://en.wikipedia.org/wiki/Generative_adversarial_network">Generative Adversarial Network (GAN)&lt;/a>&lt;sup>W&lt;/sup>: A set of two or more neural networks that can generate new data based on existing training data.
A simple example is &lt;a href="https://thispersondoesnotexist.com/">https://thispersondoesnotexist.com/&lt;/a>, that can generate fake pictures of humans.
- GANs consist of one generator network with the goal to make realistic new data, and a distinction network with the goal to tell real data from fake. As the networks train, they each improve at their individual task, forcing their adversaries to improve in turn.&lt;/p>
&lt;p>&lt;a href="https://en.wikipedia.org/wiki/Genetic_algorithm">Genetic Algorithms&lt;/a>&lt;sup>W&lt;/sup>: Algorithms that try to mimic the evolutionary process by randomly modifying the best-performing sets of parameters while discarding those with the worst performance, then repeating.&lt;/p>
&lt;p>&lt;a href="https://openai.com/blog/better-language-models/">GPT-2/3&lt;/a>: A language model from OpenAI that can generate text that mimics a writing style incredibly well.&lt;/p>
&lt;p>&lt;a href="https://en.wikipedia.org/wiki/Gradient_descent">Gradient Descent&lt;/a>&lt;sup>W&lt;/sup>: An interative process that attempts to find a minimum of a function that works by moving in the direction that will decrease the gradient until a local minima is reached.&lt;/p>
&lt;p>&lt;a href="https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning)">Hyperparameters&lt;/a>&lt;sup>W&lt;/sup>: Manually defined parameters of the model, such as the size of a neural network, or manually defined parameters of the machine learning algorithm, such as learning rate.&lt;/p>
&lt;p>&lt;a href="https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables">(IID) Independent and Identically Distributed&lt;/a>&lt;sup>W&lt;/sup>: Random variables are independent and identically distributed when the value of one variable doesn&amp;rsquo;t affect the probability of another variable. For example, the outcome of one coin flip does not affect the outcome of another coin flip, so both flips are IID.&lt;/p>
&lt;p>&lt;a href="https://dibyaghosh.com/blog/probability/kldivergence.html">KL Divergence&lt;/a>: Divergence between two distributions of data. Useful for determining how different fake data is from real data (GANs) or for determining how differemt two policies are for trust-based reinforcement learning.&lt;/p>
&lt;p>&lt;a href="https://en.wikipedia.org/wiki/Self-driving_car">Level 0-5&lt;/a>&lt;sup>W&lt;/sup>: The different &amp;ldquo;levels&amp;rdquo; of autonomy related to self driving cars. 0 represents full human control while 5 represents full vehicle control.&lt;/p>
&lt;p>&lt;a href="https://en.wikipedia.org/wiki/Long_short-term_memory">Long Short Term Memory (LSTM)&lt;/a>&lt;sup>W&lt;/sup>: A type of recurrent neural network that works exceptionally well with sequential input. A unique trait of these networks are their memory cells&amp;rsquo; &amp;ldquo;forget gates&amp;rdquo;, which allow them to control how long they hold onto data for.&lt;/p>
&lt;p>&lt;a href="https://deepai.org/publication/the-lottery-ticket-hypothesis-training-pruned-neural-networks">The Lottery Ticket Hypothesis&lt;/a>: A randomly-initialized, dense neural network contains a subnetwork that is initialised such that — when trained in isolation — it can match the test accuracy of the original network after training for at most the same number of iterations.&lt;/p>
&lt;p>&lt;a href="https://en.wikipedia.org/wiki/Markov_decision_process">Markov Decision Process&lt;/a>&lt;sup>W&lt;/sup>: A system of states, actions, and probabilities of getting to other states given actions taken from previous states.&lt;/p>
&lt;p>&lt;a href="https://aiden.nibali.org/blog/2017-01-18-mode-collapse-gans/">Mode Collapse&lt;/a>: When the distribution of samples produced by a generative adversarial network represent a subset of the latent distribution, instead of the entire latent distribution. For example, you train a network to produce pictures of animals but it only learns to produce pictures of cats.&lt;/p>
&lt;p>&lt;a href="https://en.wikipedia.org/wiki/Multi-armed_bandit">Multi-armed Bandit&lt;/a>&lt;sup>W&lt;/sup>: A core component of reinforcement learning, the multi-armed bandit problem is the classic &amp;ldquo;exploration versus exploitation&amp;rdquo; tradeoff. In this problem, expected gain must be maximized in an environment with varying rewards, forcing an agent to decide between keeping an option they know to be safe versus exploring new options that might be better.&lt;/p>
&lt;p>&lt;a href="https://stats.stackexchange.com/questions/897/online-vs-offline-learning">Online/Offline Learning&lt;/a>: Online learning happens as data comes in. Offline learning happens after data is collected and made into a batch.&lt;/p>
&lt;p>&lt;a href="http://mlss.tuebingen.mpg.de/2015/slides/ghahramani/gp-neural-nets15.pdf">Parametric &amp;amp; Non-Parametric Models&lt;/a>: Parametric models use a finite number of parameters, like weights in linear regression, to represent a learned hypothesis. Non-Parametric models use variable/infinite/no parameters, like data points in nearest neighbors, to represent a learned hypothesis.&lt;/p>
&lt;p>&lt;a href="https://en.wikipedia.org/wiki/Precision_and_recall">Precision&lt;/a>&lt;sup>W&lt;/sup>: The fraction of relevant instances among the retrieved instances. Also known as positive predictive value.&lt;/p>
&lt;p>&lt;a href="https://en.wikipedia.org/wiki/Precision_and_recall">Recall&lt;/a>&lt;sup>W&lt;/sup>: The fraction of relevant instances that were retrieved. Also known as sensitivity.&lt;/p>
&lt;p>&lt;a href="https://en.wikipedia.org/wiki/Recurrent_neural_network">Recurrent Neural Network (RNN)&lt;/a>&lt;sup>W&lt;/sup>: A type of network that can store state, giving it a type of memory that can process a series of inputs. This can be accomplished by having a cycle within the network.&lt;/p>
&lt;p>&lt;a href="https://en.wikipedia.org/wiki/Regression_analysis">Regression&lt;/a>&lt;sup>W&lt;/sup>: A set of models that determine relationships between data and a dependent value. For example, linear regression tries to approximate a dependent value while logistic regression tries to determine the probability of a dependent value.&lt;/p>
&lt;p>&lt;a href="https://en.wikipedia.org/wiki/Residual_neural_network">Residual Neural Network&lt;/a>&lt;sup>W&lt;/sup>: Networks with connections that skip some layers and connect to non-adjacent ones. This type of network helps counter the vanishing gradient problem&lt;/p>
&lt;p>&lt;a href="https://en.wikipedia.org/wiki/Reinforcement_learning">Reinforcement Learning&lt;/a>&lt;sup>W&lt;/sup>: Algorithms are given a world state they are able to interact with, and learn from the reward their interactions give them.&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://ai.stackexchange.com/questions/4456/whats-the-difference-between-model-free-and-model-based-reinforcement-learning">Model-Based&lt;/a>: You create a model of the world and can predict what the next state and reward will be for each action&lt;/li>
&lt;li>&lt;a href="https://ai.stackexchange.com/questions/4456/whats-the-difference-between-model-free-and-model-based-reinforcement-learning">Model-Free&lt;/a>: You know what action to take without knowing what to expect, since you don&amp;rsquo;t have a model of the world&lt;/li>
&lt;li>&lt;a href="https://www.freecodecamp.org/news/an-intro-to-advantage-actor-critic-methods-lets-play-sonic-the-hedgehog-86d6240171d/">Value&lt;/a>: Networks that determine the value of a state.&lt;/li>
&lt;li>&lt;a href="https://www.freecodecamp.org/news/an-intro-to-advantage-actor-critic-methods-lets-play-sonic-the-hedgehog-86d6240171d/">Policy&lt;/a>: Network to choose actions. Can directly optimize model instead of computing values, useful when you have a continuous action space. Only uses reward function. Requires a score function to evaluate performance of policy, usually total rewards accumulated given a period of time.&lt;/li>
&lt;li>&lt;a href="https://www.freecodecamp.org/news/an-intro-to-advantage-actor-critic-methods-lets-play-sonic-the-hedgehog-86d6240171d/">Actor Critic&lt;/a>: Backbone of state of the art reinforcement learning algorithms.
&lt;ul>
&lt;li>Uses a value-based Critic to measure how good the action taken is&lt;/li>
&lt;li>Uses a policy-based Actor to to choose actions&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;a href="https://en.wikipedia.org/wiki/Stochastic">Stochastic&lt;/a>&lt;sup>W&lt;/sup>: Randomly determined process, usually refers to probabilistic outputs of machine learning systems&lt;/p>
&lt;p>&lt;a href="https://en.wikipedia.org/wiki/Supervised_learning">Supervised Learning&lt;/a>&lt;sup>W&lt;/sup>: A model learns to produce a desired output and knows what that output is. Example: Image Recognition&lt;/p>
&lt;p>&lt;a href="https://en.wikipedia.org/wiki/Temporal_difference_learning">Temporal Difference&lt;/a>&lt;sup>W&lt;/sup>: Model-free reinforcement learning design which learns by bootstrapping value samples in order to approximate a value function. Once more information is revealed about the true value during later timesteps, you can update the low information bootstrapped guess by using the newly acquired outcome as a &amp;ldquo;ground truth&amp;rdquo; to train a network.&lt;/p>
&lt;ul>
&lt;li>Temporal Difference error is the difference between consecutive temporal predictions.&lt;/li>
&lt;/ul>
&lt;p>&lt;a href="https://ai.stackexchange.com/questions/7359/what-is-a-trajectory-in-reinforcement-learning">Trajectories&lt;/a>: The history of states (and potentially actions) taken during a walk of a Markov Decision Process.&lt;/p>
&lt;p>&lt;a href="https://en.wikipedia.org/wiki/Transfer_learning">Transfer Learning&lt;/a>&lt;sup>W&lt;/sup>: Taking parts of a network trained on one data set, and using it in a different network with a different dataset.&lt;/p>
&lt;p>&lt;a href="https://towardsdatascience.com/transformers-141e32e69591">Transformer&lt;/a>: A type of recurrent neural network primarily used with sequential data, like language translation. These networks use an attention model to improve performance.&lt;/p>
&lt;p>&lt;a href="https://en.wikipedia.org/wiki/Unsupervised_learning">Unsupervised Learning&lt;/a>&lt;sup>W&lt;/sup>: A model learns to produce a desired output without being told what it&amp;rsquo;s looking for. Example: Playing Chess&lt;/p>
&lt;p>&lt;a href="https://en.wikipedia.org/wiki/Vanishing_gradient_problem">Vanishing Gradient Problem&lt;/a>&lt;sup>W&lt;/sup>: In a network, a gradient can become vanishingly small which will stop the weight from changing it&amp;rsquo;s value, since weights are modified based on their contribution to the gradient.&lt;/p>
&lt;p>Feel free to contact me with any suggested additions/changes at &lt;a href="mailto:jarbus@tutanota.com">jarbus@tutanota.com&lt;/a>.&lt;/p></description></item><item><title>Generating Slides in Vim</title><link>/blog/generating-slides-in-vim/</link><pubDate>Sun, 31 Jan 2021 11:25:49 -0400</pubDate><guid>/blog/generating-slides-in-vim/</guid><description>&lt;p>There is a great tool known as &lt;a href="https://pandoc.org/">pandoc&lt;/a> that can convert documents from one filetype to another. For example, you can convert a Microsoft Word document to a PDF, without even needing to own a copy of Microsoft Word! However, we care about Pandoc&amp;rsquo;s ability to convert a Markdown document to a slideshow presentation using LaTeX Beamer as a rendering engine.&lt;/p>
&lt;p>There is a great writeup about this basic feature &lt;a href="https://ashwinschronicles.github.io/beamer-slides-using-markdown-and-pandoc">here&lt;/a>.&lt;/p>
&lt;blockquote>
&lt;p>TL;DR: With Pandoc installed and markdown file &lt;code>Demo.md&lt;/code>, executing &lt;code>pandoc -t beamer Demo.md -o Demo.pdf&lt;/code> will generate a slideshow as a pdf, with each section heading as the title of a new slide.&lt;/p>&lt;/blockquote>
&lt;p>Different themes can be specified with the &lt;code>-V&lt;/code> option; I&amp;rsquo;m currently using the &lt;a href="https://github.com/matze/mtheme">Metropolis&lt;/a> beamer theme and &lt;a href="https://github.com/rchurchley/beamercolortheme-owl">Owl&lt;/a> colorscheme. After installing these themes, I can run:&lt;/p>
&lt;pre tabindex="0">&lt;code>pandoc -t beamer
-V fontsize=14pt
-V theme=metropolis
-V colortheme=owl
-s
Demo.md -o Demo.pdf
&lt;/code>&lt;/pre>&lt;p>to produce my presentation. &lt;strong>Note: All code blocks are single-line commands/&lt;code>.vimrc&lt;/code> entries, and are displayed as multi-line for readability.&lt;/strong>&lt;/p>
&lt;p>Unfortunately, it&amp;rsquo;s a pain to recompile a document every time I want to view my changes. Luckily, as a (Neo)Vim user, I can automatically run commands everytime I write a file. To avoid generating presentations with non-presentation markdown files, I instead write presentations in plaintext files that end with &lt;code>.slides&lt;/code>. In the example above, I&amp;rsquo;d be using &lt;code>Demo.slides&lt;/code> instead of &lt;code>Demo.md&lt;/code>. When I add:&lt;/p>
&lt;pre tabindex="0">&lt;code>autocmd! BufWritePost *.slides silent !pandoc
-t beamer
-V fontsize=14pt
-V theme=metropolis
-V colortheme=owl
-s
% -o &amp;#34;%:p:h/.%:t:r.pdf&amp;#34;
&lt;/code>&lt;/pre>&lt;p>to my &lt;code>.vimrc&lt;/code>, everytime I save changes in a &lt;code>FILENAME.slides&lt;/code> file, it will generate a presentation located in &lt;code>.FILENAME.pdf&lt;/code>. I choose to add a leading period in the output to hide the PDF from my file manager, feel free to remove it.&lt;/p>
&lt;p>To enable markdown syntax highlighting in our &lt;code>.slides&lt;/code> file, add:&lt;/p>
&lt;pre tabindex="0">&lt;code>autocmd BufRead,BufNewFile *.slides :set filetype=markdown
&lt;/code>&lt;/pre>&lt;p>to your &lt;code>.vimrc&lt;/code>.&lt;/p>
&lt;p>Now Vim generates a PDF presentation for us everytime we save a &lt;code>.slides&lt;/code> file. For easy viewing, I added another keybind, &lt;code>Z&lt;/code>, to open the corresponding PDF in my PDF viewer of choice, &lt;a href="https://pwmt.org/projects/zathura/">Zathura&lt;/a>:&lt;/p>
&lt;pre tabindex="0">&lt;code>nnoremap Z :silent !zathura --fork %:p:h/.%:t:r.pdf&amp;lt;CR&amp;gt;
&lt;/code>&lt;/pre>&lt;p>Zathura automatically refreshes the view whenever changes to an open document is detected. This means that I can open a &lt;code>.slides&lt;/code> file in Vim, save changes, and press &lt;code>Z&lt;/code> to view my changes in Zathura. I can then immediately view any additional changes I make in the open Zathura window, just by saving the &lt;code>.slides&lt;/code> file.&lt;/p></description></item><item><title>Tesla and False Advertising in AI</title><link>/blog/tesla-and-false-advertising-in-ai/</link><pubDate>Wed, 22 Jul 2020 11:25:49 -0400</pubDate><guid>/blog/tesla-and-false-advertising-in-ai/</guid><description>&lt;p>Here&amp;rsquo;s the problem with advertising AI-based technology that doesn&amp;rsquo;t exist:&lt;/p>
&lt;p>&lt;strong>You cannot promise anything about your product.&lt;/strong>&lt;/p>
&lt;p>We&amp;rsquo;ve all seen AI advertised to the masses that doesn&amp;rsquo;t work as advertised, just look at any voice-to-text system. When I got my Apple Watch, I hoped to use it to respond to messages without getting distracted by my phone. I quickly realized that wasn&amp;rsquo;t a viable solution: I had to repeat my message multiple times per text in order to get the correct dictation.&lt;/p>
&lt;p>If this was a software issue, I could file a bug report and Apple could send out a fix. Unfortunately with AI, there isn&amp;rsquo;t much Apple can do besides get more data and re-train the model, which still wouldn&amp;rsquo;t guarantee a better user experience for anyone. Mind you, this is with software that&amp;rsquo;s already shipped.&lt;/p>
&lt;p>Now let&amp;rsquo;s look at Tesla, or more specifically its CEO, Elon Musk. &lt;a href="https://futurism.com/elon-musk-tesla-close-level-5-autonomy">Musk claims that Level 5 driving (in which a human is not needed behind the wheel) will be coming to Tesla vehicles in the form of a software update by the end of 2020&lt;/a>. A month before he made said claim, &lt;a href="https://futurism.com/the-byte/tesla-autopilot-slam-overturned-truck">a Tesla on autopilot crashed into a flipped truck&lt;/a>&amp;ndash;a mistake most humans wouldn&amp;rsquo;t make.&lt;/p>
&lt;p>Just like Apple can&amp;rsquo;t promise its speech-to-text system will work better than a human (or even better than a toddler), Musk can&amp;rsquo;t guarantee anything about Tesla&amp;rsquo;s Level 5 capability. Tesla can&amp;rsquo;t even &amp;ldquo;reasonably claim&amp;rdquo; your car won&amp;rsquo;t crash into flipped trucks or &lt;a href="https://www.msn.com/en-us/autos/enthusiasts/another-tesla-crashes-into-a-cop-car-while-on-autopilot/ar-BB16Mvkk#!">stopped cop cars&lt;/a>. In fact, no one using deep learning can promise anything about their system. Deep learning, and by extension Tesla&amp;rsquo;s Autopilot system, is a black box. We don&amp;rsquo;t know why it works, we can&amp;rsquo;t promise that it&amp;rsquo;ll work, but it seems to work when we test it. Unless Tesla is testing on all roads in all conditions, they shouldn&amp;rsquo;t even be &amp;ldquo;confident&amp;rdquo; level 5 technology, in which &lt;strong>a human does not need to be in the vehicle&lt;/strong>, is &amp;ldquo;around the corner&amp;rdquo;. That&amp;rsquo;s why &lt;a href="https://www.reuters.com/article/us-tesla-autopilot-germany/german-court-bans-tesla-ad-statements-related-to-autonomous-driving-idUSKCN24F1T5">a court in Germany banned Tesla from including “full potential for autonomous driving” and “Autopilot inclusive” in its German advertising materials&lt;/a>.&lt;/p>
&lt;p>A Full Self-Driving (FSD) update wouldn&amp;rsquo;t be the first time Tesla over-promises and under-delivers. Consumer Reports described the company&amp;rsquo;s &amp;ldquo;Smart Summon&amp;rdquo; feature as &lt;a href="https://www.consumerreports.org/automotive-technology/teslas-smart-summon-performance-doesnt-match-marketing-hype/">&amp;ldquo;glitchy and at times worked intermittently&amp;rdquo;&lt;/a>. The damage with Smart Summon has been low, but a full-autonomy update could result in a few consequences:&lt;/p>
&lt;ul>
&lt;li>Tesla sold cars with the promise of Level 5 driving in the future: Not only is Tesla now under immense pressure to rush out a Level 5 update as soon as possible, but must do so using the hardware it sold customers years ago. Hopefully Level 5 doesn&amp;rsquo;t require LIDAR sensors (or any additional hardware) like Elon claims; because Tesla is expected to ship some version of FSD to users.&lt;/li>
&lt;li>Customers may end up in legal trouble if Tesla&amp;rsquo;s software fails them: Tesla avoids legal responsibility for buggy software by shifting responsibility of an accident to the driver. However, when Tesla sells millions of cars on the premise that users won&amp;rsquo;t have to manually drive them one day, should drivers be held responsible when that day comes and their car crashes?&lt;/li>
&lt;/ul>
&lt;p>Other companies have realized the high standards a Level 5 system should meet. Two years ago, &lt;a href="https://www.bloomberg.com/news/articles/2018-11-13/waymo-ceo-says-self-driving-cars-won-t-be-ubiqitious-for-decades">Waymo CEO John Krafcik claimed that true self-driving cars, that function everywhere under all circumstances, won&amp;rsquo;t be ubiquitous for decades&lt;/a>. As a result, Waymo shifted gears and placed its focus on rolling out &lt;a href="https://www.theverge.com/2019/12/9/21000085/waymo-fully-driverless-car-self-driving-ride-hail-service-phoenix-arizona">autonomous taxis&lt;/a> on a city by city basis, starting in Pheonix, Arizona. This approach to Level 5 driving is much safer for the following reasons:&lt;/p>
&lt;ul>
&lt;li>Waymo is minimizing the number of people at risk&lt;/li>
&lt;li>Waymo is operating its vehicles strictly where they have been tested.&lt;/li>
&lt;li>Waymo is not selling consumers a technology that doesn&amp;rsquo;t exist&lt;/li>
&lt;li>Waymo is not relying on the average consumer to be attentive&lt;/li>
&lt;/ul>
&lt;p>Level 5 technology can save thousands of lives and millions of dollars&amp;ndash;but it needs to be done to the same (if not higher) standards of automotive testing, and not the &amp;ldquo;move fast and break things&amp;rdquo; approach of Silicon Valley. Hopefully Tesla can safely deliver on its promise of full self driving soon, but if not, consumers may soon have a case to demand a refund.&lt;/p></description></item><item><title>Bitcoin Whitepaper, Explained</title><link>/blog/bitcoin-whitepaper-explained/</link><pubDate>Fri, 03 Jan 2020 11:25:49 -0400</pubDate><guid>/blog/bitcoin-whitepaper-explained/</guid><description>&lt;h2 id="introduction" >Introduction
&lt;span>
&lt;a href="#introduction">
&lt;svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg">&lt;path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/>&lt;path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/>&lt;/svg>
&lt;/a>
&lt;/span>
&lt;/h2>&lt;p>In this essay we will give a brief history of crytocurrency leading up to
Bitcoin, give an overview of the Bitcoin protocol by summarizing
key sections of the whitepaper, and briefly discuss Bitcoin&amp;rsquo;s use of cryptographical
proof and computational security instead of trusted third parties within
the protocol.&lt;/p>
&lt;h2 id="history" >History
&lt;span>
&lt;a href="#history">
&lt;svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg">&lt;path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/>&lt;path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/>&lt;/svg>
&lt;/a>
&lt;/span>
&lt;/h2>&lt;p>Bitcoin is far from original. Digital currencies date back as far as
1982, where David Chaum released a paper called &amp;quot;Blind Signatures&amp;quot;,
which formed the basis of a digital currency known as eCash. In 1996,
Douglass Jackson created a currency known as e-gold in 1996, the first
currency to reach mass adoption at over 3.5 million accounts.
Many other subsequent currencies rose and fell before Bitcoin gained
stable footing, including projects such as BitGold and b-money.&lt;/p>
&lt;p>Blockchain technology development began as early as 1991 when Stuart
Haber and W. Scott Stornetta began work on a cryptographically secured
chain of blocks. &lt;a href="https://link.springer.com/content/pdf/10.1007%2F3-540-38424-3_32.pdf">Their inital goal was to implement a system where
document timestamps could not be tampered with.&lt;/a> Bitcoin builds upon
Haber and Stornetta&amp;rsquo;s subsequent work of reliable timestamping and
securing names for bit strings and cites three of their papers,
making up a third of the nine citations in the Bitcoin whitepaper.&lt;/p>
&lt;p>Bitcoin also builds upon technology such as Proof-of-Work, first
developed in 1997 by Adam Back under the name &lt;a href="http://hashcash.org/">Hashcash.&lt;/a>
Proof-of-Work is a counter-measure towards abuse of un-metered internet
resources which could enable a Denial of Service attack on email by
requiring an attacker to expend computational resources in order to
interact with the network. A reusable version of Proof-Of-Work was
further developed by Hal Finney in 2004, the receiver of the first
bitcoin transaction. Proof-Of-Work will be further discussed later on in
this paper.&lt;/p>
&lt;p>Satoshi Nakamoto, the anonymous creator(s) of Bitcoin released the
Bitcoin whitepaper to a cryptography mailing list on October 31st, 2008,
four short years after Hal Finney&amp;rsquo;s paper. The first block was mined
January the following year, with a comment that reads:&lt;/p>
&lt;blockquote>
&lt;p>&amp;ldquo;The Times 03/Jan/2009 Chancellor on brink of second bailout for banks&amp;rdquo;&lt;/p>&lt;/blockquote>
&lt;p>Very fitting for the anti-authority ideals Bitcoin would spur in the coming years.&lt;/p>
&lt;h2 id="design" >Design
&lt;span>
&lt;a href="#design">
&lt;svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg">&lt;path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/>&lt;path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/>&lt;/svg>
&lt;/a>
&lt;/span>
&lt;/h2>&lt;h3 id="section-2-and-9-transactions-combining-and-splitting-value" >Section 2 and 9: Transactions, Combining and Splitting Value
&lt;span>
&lt;a href="#section-2-and-9-transactions-combining-and-splitting-value">
&lt;svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg">&lt;path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/>&lt;path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/>&lt;/svg>
&lt;/a>
&lt;/span>
&lt;/h3>&lt;p>Bitcoin is represented as a publicly visible global log of
&lt;a href="https://en.wikipedia.org/wiki/Digital_signature">signed&lt;/a>$^W$
transactions. Each transaction denotes the participants in a transaction
as well as the amount of money transfered (analogous to the Venmo public
feed), and the signature ensures the account publishing the transaction
is the account who owns the Bitcoin. Since the amount of Bitcoin an
account has is directly represented by outputs of previous transactions,
sending money is represented by taking previous transactions addressed
to your account, sigining them, and specifying which addresses will
receive which portions of the funds. Since one or more transactions can
be used as input for this process, there can be one or more addresses
receiving the output, including the spender who recieves any left-over
Bitcoin as change, since it&amp;rsquo;s impossible to use a fraction of a
transaction as input.&lt;/p>
&lt;p>Transactions must be verified, i.e. nodes must confirm that transaction
signatures come from their corresponding private keys. Transactions are
verified in large batches. Each transaction is added to a pool of
unverified transactions, which are then all verified and published to
the network in a block, approximately once every ten minutes. Each block
is published with a timestamp and a hash of a previous block, forming a
&amp;ldquo;chain&amp;rdquo; of all valid transactions.&lt;/p>
&lt;h3 id="sections-3-and-4-timestamp-server-proof-of-work" >Sections 3 and 4: Timestamp server, Proof of Work
&lt;span>
&lt;a href="#sections-3-and-4-timestamp-server-proof-of-work">
&lt;svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg">&lt;path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/>&lt;path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/>&lt;/svg>
&lt;/a>
&lt;/span>
&lt;/h3>&lt;p>Bitcoin also implements a distributed timestamp server. Normal timestamp
servers take hashes of previous timestamps and the current block in
order to form a new timestamp hash, then widely publish this hash.
However, this requires a trusted party, and does not work in a
distributed setting with no trusted parties.&lt;/p>
&lt;p>Nakamoto takes inspiration from Adam Back&amp;rsquo;s Hashcash algorithm, which
implements a Proof-Of-Work scheme based off of finding a hash with a
specific number of zeros at the beginning. This is a very
computationally expensive task, as the only method of obtaining such a
hash is brute force. Since a new block is published every ten minutes
and each block requires the hash of a previous block, this requires
anyone attmempting to modify the blockchain history to not only find the
proper hash at the specific time, but also all hashes of subsequent
blocks in order to maintain the validity of the ledger. This type of
attack can be used to spend the same Bitcoin twice, as a user could make
a transaction, erase it from the blockchain, and then use it again.
However, thanks to the proof-of-work algorithm, this attack increases in
difficultly exponentially as new blocks are added, and is one of the
many forms of computational security the Bitcoin protocol implements.
Since the amount of computational power on the blockchain will naturally
vary overtime, the proof-of-work algorithm will also adjust to
compensate for increased/decreased mining power in order to keep blocks
published approximately every ten minutes.&lt;/p>
&lt;p>The first block verified and published is added to the network, and the
longest chain of blocks is deemed to be the correct one.&lt;/p>
&lt;h3 id="section-5-network" >Section 5: Network
&lt;span>
&lt;a href="#section-5-network">
&lt;svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg">&lt;path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/>&lt;path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/>&lt;/svg>
&lt;/a>
&lt;/span>
&lt;/h3>&lt;p>The Bitcoin network runs on the following steps taken verbatim from the
original whitepaper, as there is not much summary for this section I
deem myself capable of doing better than Nakamoto.&lt;/p>
&lt;ol>
&lt;li>New transactions are broadcast to all nodes&lt;/li>
&lt;li>Each node collects new transactions into a block&lt;/li>
&lt;li>Each node works on finding a difficult proof-of-work for its block&lt;/li>
&lt;li>When a node finds a proof-of-work, it broadcasts the block to all nodes&lt;/li>
&lt;li>Nodes accept the block only if all transactions in it are valid and
not already spent&lt;/li>
&lt;li>Nodes express their acceptance of the block by working on creating
the next block in the chain, using the hash of the accepted block as
the previous hash.&lt;/li>
&lt;/ol>
&lt;p>Nodes always consider the longest chain to be the correct one and will
keep working on extending it.&lt;/p>
&lt;h3 id="sections-6-8-incentive-reclaiming-disk-space-simplified-payment-verification" >Sections 6-8: Incentive, Reclaiming Disk Space, Simplified Payment Verification
&lt;span>
&lt;a href="#sections-6-8-incentive-reclaiming-disk-space-simplified-payment-verification">
&lt;svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg">&lt;path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/>&lt;path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/>&lt;/svg>
&lt;/a>
&lt;/span>
&lt;/h3>&lt;p>Section six explains the financial benefits of mining, namely how miners get newly minted coins and transaction fees.
Nakamoto then explains how even when an adversary can claim control over a majority of the network&amp;rsquo;s mining power, it makes economic sense to act according to the protocol to gain profit instead of creating double-spend attacks, since people will stop using the system once it is compromised.&lt;/p>
&lt;p>In section seven, Nakamoto also discusses storage optimizations for
reducing the amount of disk space the entire blockchain uses. The
primary method discussed involved removing transactions that are buried
underneath many blocks but hashing the transactions in a Merkle tree as
to break the hashes of any blocks.&lt;/p>
&lt;p>In section eight, Nakamoto describes a method of payment verification
without needing to run a full node, as well as potential attacks and
countermeasures on nodes attempting to verify transactions without
storing the entire blockchain.&lt;/p>
&lt;h3 id="section-10-privacy" >Section 10: Privacy
&lt;span>
&lt;a href="#section-10-privacy">
&lt;svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg">&lt;path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/>&lt;path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/>&lt;/svg>
&lt;/a>
&lt;/span>
&lt;/h3>&lt;p>It should be noted that Bitcoin has no privacy protections in place to
protect users besides basic anonymity of not tying real-world identities
to keys. The amount of Bitcoin sent in each transaction is public, as
well as the public keys used. New public-private keypairs are
recommended for each transaction, but the multi-input nature of
transactions reveals two inputs are attached to the same owner. As a
result, if the identity of a sender is compromised, it&amp;rsquo;s possible to
trace transaction history and link previous transactions that user has
made.&lt;/p>
&lt;h3 id="section-11-cryptography-and-security" >Section 11: Cryptography and Security
&lt;span>
&lt;a href="#section-11-cryptography-and-security">
&lt;svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg">&lt;path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/>&lt;path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/>&lt;/svg>
&lt;/a>
&lt;/span>
&lt;/h3>&lt;p>Attackers are unable to steal the money of other users without their
private key. There is also no method to generate new Bitcoin in anyway
that an honest node could recognize as valid. Therefore, the only
feasible attack on Bitcoin is to generate an alternative chain of
blocks. Since the longest chain of blocks is considered valid, this
attack requires the generation of an alternative chain faster than the
honest chain. This can be thought of as a race between the attacker and
the rest of the network, with the attacker starting from behind since
they have to re-compute proof of work for existing blocks.&lt;/p>
&lt;p>Nakamoto demonstrates how an attacker with $51%$ of the networks hashing power
will be guaranteed to eventually catch up and sustain an alternative
chain, however given a sufficiently large network Nakamoto assumes this
infeasilbe, as the computation power required would be enormous.
Additionally, every time an adversary fails to beat an honest node in
creating the next block, the amount of blocks they need to catch up to
increases and thus unless an attacker gets incredibly lucky when they
first start generating an alternate chain, their chances of catching up
to the main chain decreases exponentially. Since the chance for an
attacker to catch up to the honest chain decreases exponentially as new
blocks are added, Nakamoto demonstrates how long an honest recipient
must wait before they can be 99.9% certain a double-spend attack will
not happen given attackers of various computational power. For example, if attackers
control 10% of the mining power, one would need to merely wait five
blocks, but if attackers have 45%, one would need to wait 340 blocks,
or just under two and a half days, to be sure their transaction is safe.&lt;/p>
&lt;h3 id="resources-used" >Resources Used
&lt;span>
&lt;a href="#resources-used">
&lt;svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg">&lt;path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/>&lt;path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/>&lt;/svg>
&lt;/a>
&lt;/span>
&lt;/h3>&lt;p>Of course the &lt;a href="https://bitcoin.org/bitcoin.pdf">Bitcoin whitepaper&lt;/a> and it&amp;rsquo;s references have been used in the writing of this essay, but I also wanted to credit &lt;a href="https://www.quora.com/What-came-before-bitcoin">this Quora post&lt;/a> for pointing author in the right direction for the second section.&lt;/p></description></item></channel></rss>