<!doctype html><html lang=en-us data-theme><head><link rel=preload href=../../unifont-small.woff2 as=font type=font/woff2 crossorigin=anonymous><meta charset=utf-8><meta name=HandheldFriendly content="True"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer-when-downgrade"><a rel=me href=https://fosstodon.org/@jarbus></a><title>From REINFORCE to R1: an Abridged Genealogy of Reinforcement Learning - </title><meta name=description content="Starting from REINFORCE, the original deep reinforcement learning algorithm, we will trace the evolution of policy gradient methods to the Group Relative Policy Optimization algorithm used to train Deepseek r1.
This post ignores the LLM side of things, less-related developments in RL, and most of the equations used for these algorithms,  but captures the essence and intuition of the RL-timeline without wasting your time. This is all self-study, so feel free to send me any corrections/suggestions.1"><link rel=icon type=image/png href=../../favicon.png><link rel=apple-touch-icon-precomposed href=../../favicon.png><style>body{visibility:hidden;opacity:0}</style><noscript><style>body{visibility:visible;opacity:1}</style></noscript><link rel=stylesheet href=../../css/style.min.86e95aed90b2c13a36036bcd828b55c65e25e8250f2b655d072838f66cb51223.css integrity="sha256-hula7ZCywTo2A2vNgotVxl4l6CUPK2VdByg49my1EiM="><script src=../../js/script.min.74bf1a3fcf1af396efa4acf3e660e876b61a2153ab9cbe1893ac24ea6d4f94ee.js type=text/javascript integrity="sha256-dL8aP88a85bvpKzz5mDodrYaIVOrnL4Yk6wk6m1PlO4="></script><meta property="og:url" content="/blog/from-reinforce-to-r1-an-abridged-genealogy/"><meta property="og:title" content="From REINFORCE to R1: an Abridged Genealogy of Reinforcement Learning"><meta property="og:description" content="Starting from REINFORCE, the original deep reinforcement learning algorithm, we will trace the evolution of policy gradient methods to the Group Relative Policy Optimization algorithm used to train Deepseek r1.
This post ignores the LLM side of things, less-related developments in RL, and most of the equations used for these algorithms, but captures the essence and intuition of the RL-timeline without wasting your time. This is all self-study, so feel free to send me any corrections/suggestions.1"><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="blog"><meta property="article:published_time" content="2025-02-21T19:58:43-05:00"><meta property="article:modified_time" content="2025-02-21T19:58:43-05:00"><meta property="article:tag" content="Ai"><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}},window.addEventListener("load",e=>{document.querySelectorAll("mjx-container").forEach(function(e){e.parentElement.classList+="has-jax"})})</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script></head><body><a class=skip-main href=#main>Skip to main content</a><div class=container><header class=common-header><div class=header-top><h1 class=site-title><a href=../../></a></h1></div><img src=../../dithered-tree-contrast.gif alt="Dithered Tree" style="display:block;margin:0 auto;max-height:20vh"><nav><a href=../../ title=home>~jarbus</a>
<a href=../../blog/ title=posts>posts</a>
<a href=../../list title=list>list</a>
<a href=../../index.xml title=Rss>rss</a></nav></header><main id=main tabindex=-1><article class="post h-entry"><div class=post-header><header><h1 class="p-name post-title">From REINFORCE to R1: an Abridged Genealogy of Reinforcement Learning</h1></header></div><div class="content e-content"><p>Starting from REINFORCE, the original deep reinforcement learning algorithm, we will trace the evolution of policy gradient methods to the Group Relative Policy Optimization algorithm used to train <a href=https://github.com/deepseek-ai/DeepSeek-R1>Deepseek r1</a>.</p><p>This post ignores the LLM side of things, less-related developments in RL, and most of the equations used for these algorithms, but captures the essence and intuition of the RL-timeline without wasting your time. This is all self-study, so feel free to send me any corrections/suggestions.<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup></p><h1 id=reinforcement-learning-101>Reinforcement Learning 101
<span><a href=#reinforcement-learning-101><svg viewBox="0 0 28 23" height="100%" width="19"><path d="M10 13a5 5 0 007.54.54l3-3a5 5 0 00-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/><path d="M14 11a5 5 0 00-7.54-.54l-3 3a5 5 0 007.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/></svg></a></span></h1><p>Reinforcement learning is a subfield of machine learning that can learn without human data. Instead, an agent can learn through interacting with its environment by taking actions and receiving rewards based on its actions. The agent wants to maximize its reward.</p><p>Specifically, an environment at time $t$ with state $s_t$ sends an observation, $o_t$, to an agent, which produces probabilities to select an action, $a_t$, based on the observation. The environment transitions to a new state, $s_{t+1}$, and the agent receives a reward, $r_{t+1}$, based on the action it took. The agent&rsquo;s goal is to learn a policy, $\pi(a_t|o_t)$ that maximizes the expected sum of rewards, $R = \sum_{t=0}^T \gamma^t r_t$, where $T$ is the number of steps in the trajectory (sequence of states, actions, and rewards from the start to the end of an episode), and $\gamma$ is a discount factor, which reduces the value of rewards the farther into the future they are. There are ways to perform reinforcement learning without neural networks, but we will focus on deep reinforcement learning, where all policy actions are determined by a neural network with parameters $\theta$.</p><h1 id=reinforce>REINFORCE
<span><a href=#reinforce><svg viewBox="0 0 28 23" height="100%" width="19"><path d="M10 13a5 5 0 007.54.54l3-3a5 5 0 00-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/><path d="M14 11a5 5 0 00-7.54-.54l-3 3a5 5 0 007.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/></svg></a></span></h1><p>The original policy gradient, and the basis for all policy gradient methods, is the <a href=https://dilithjay.com/blog/reinforce-a-quick-introduction-with-code>REINFORCE</a> algorithm. REINFORCE samples trajectories from the environment, which it uses to directly compute the gradient of the policy:</p>$$
\theta_{t+1} = \theta_t + \alpha R_t \sum_{t=0}^T \nabla_\theta \log \pi(a_t|o_t)
$$<p>where $\alpha$ is the learning rate, and $R_t$ is the remaining discounted reward&ndash;also called a return&ndash;from the current state given by $R_t = \sum_{t&rsquo;=t}^T \gamma^{t&rsquo;-t} r_{t&rsquo;}$.</p><p>REINFORCE simply &ldquo;reinforces&rdquo; the actions proportional to the reward they receive. This is the most intuitive reinforcement learning algorithm, but it has high variance, which leads us to actor critics.</p><p>I struggled with the concept of variance in RL for a while, but it&rsquo;s essentially the amount of randomness in the reward signal. When you are trying to learn purely from experience, rewards from a given state can vary wildly between trajectories depending on how the environment plays out, so we need a lot of samples to get a rough idea of what the &ldquo;true&rdquo; reward is for a given state. Actor-critic methods reduce this variance by using another neural network called a &ldquo;critic&rdquo; to estimate the value of a state. It turns out that using a consistent estimator, like a neural network, stabilizes learning, as the learning signal doesn&rsquo;t fluctuate as much between samples.</p><h1 id=from-reinforce-to-actor-critic>From REINFORCE to Actor-Critic
<span><a href=#from-reinforce-to-actor-critic><svg viewBox="0 0 28 23" height="100%" width="19"><path d="M10 13a5 5 0 007.54.54l3-3a5 5 0 00-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/><path d="M14 11a5 5 0 00-7.54-.54l-3 3a5 5 0 007.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/></svg></a></span></h1><p>As mentioned, actor-critic methods use neural networks to estimate reward. This results in more stable learning compared to using trajectories directly. The actor is just the policy from REINFORCE, and the critic is a new, separate network that estimates the return given the current policy. We won&rsquo;t cover the training of the critic here, but just assume it&rsquo;s just a neural network that takes observations $o_i$ as input and estimates the return $R$ for the actor.</p><p>At it&rsquo;s core, actor-critic is just REINFORCE, but $R_t$ is computed by another neural network, which is trained using the actual returns from the environment.</p><p>As a bonus, we can now update actors without requiring the full trajectory, since we are using an estimate instead of the true return.</p><h1 id=from-actor-critic-to-advantage-actor-critic-a2c>From Actor-Critic to Advantage Actor-Critic (A2C)
<span><a href=#from-actor-critic-to-advantage-actor-critic-a2c><svg viewBox="0 0 28 23" height="100%" width="19"><path d="M10 13a5 5 0 007.54.54l3-3a5 5 0 00-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/><path d="M14 11a5 5 0 00-7.54-.54l-3 3a5 5 0 007.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/></svg></a></span></h1><p>The next step in the evolution of policy gradient methods is the Advantage Actor-Critic (A2C) algorithm. Now we introduce the <em>advantage function</em>, which is the difference between the return and the value estimate of the state:</p>$$
A_t = r + \gamma V(o_{t+1}) - V(o_t)
$$<p>where $V(o_i)$ is the value estimate of the state computed by our critic at time $i$.</p><p>The advantage function tells us how much better our action was compared to the expected action for that state, which allows us to reinforce actions based on how much they improve our expected reward. Conversely, this also discourages actions based on how much they decrease our expected reward. This is unlike prior methods, which might only positively (or negatively) reinforce actions based on the absolute reward they receive, not the relative reward. Without the advantage function, given an environment which only produces positive rewards, we might only reinforce good actions, without ever penalizing bad ones.</p><p>Now, the policy gradient is (roughly) computed as:</p>$$
\theta_{t+1} = \theta_t + \alpha A_t \sum_{t=0}^T \nabla_\theta \log \pi(a_t|o_t)
$$<p>where $A_t$ is the advantage function computed using the critic network. This further stabilizes learning.</p><h1 id=from-advantage-actor-critic-to-proximal-policy-optimization-ppo>From Advantage Actor-Critic to Proximal Policy Optimization (PPO)
<span><a href=#from-advantage-actor-critic-to-proximal-policy-optimization-ppo><svg viewBox="0 0 28 23" height="100%" width="19"><path d="M10 13a5 5 0 007.54.54l3-3a5 5 0 00-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/><path d="M14 11a5 5 0 00-7.54-.54l-3 3a5 5 0 007.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/></svg></a></span></h1><p>Skipping over many other developments, we jump directly from A2C to Proximal Policy Optimization (PPO). PPO uses the advantage function to update the policy, but just by a little bit, by introducing a <em>clipping</em> term that limits the size of the update. This prevents the policy from changing its behavior too much in a given update, which might otherwise destabilize training and cause the policy to forget some learned behaviors.</p><p>PPO prevents behaviors from changing too much by ensuring that the probability of taking an action in a given state doesn&rsquo;t change too much, using a ratio of the new policy to the old policy, $\frac{\pi_\theta(a_t|o_t)}{\pi_{\theta_{\text{old}}}(a_t|o_t)}$. When the probabilities don&rsquo;t change, $ \frac{\pi_\theta(a_t|o_t)}{\pi_{\theta_{\text{old}}}(a_t|o_t)}= \mathbf{1}$, but if the probabilities change, $\frac{\pi_\theta(a_t|o_t)}{\pi_{\theta_{\text{old}}}(a_t|o_t)}$ will range between 0 and $\infty$. PPO clips the ratio to a range of $[1-\epsilon, 1+\epsilon]$, where $\epsilon$ is a hyperparameter we choose between 0 and 1 to determine the max percentage by which a probability can change.</p><p>The PPO update is then:</p>$$
\theta_{t+1} = \theta_t + \alpha \nabla_\theta \mathbb{E}_t \left[ \min( \frac{\pi_\theta(a_t|o_t)}{\pi_{\theta_{\text{old}}}(a_t|o_t)}
A_t, \text{clip}(\frac{\pi_\theta(a_t|o_t)}{\pi_{\theta_{\text{old}}}(a_t|o_t)} , 1-\epsilon, 1+\epsilon) A_t) \right]
$$<p>This adds a lot of terms to our update, but hopefully the jump from A2C to PPO is visible.</p><h1 id=from-proximal-policy-optimization-to-group-relative-policy-optimization>From Proximal Policy Optimization to Group Relative Policy Optimization
<span><a href=#from-proximal-policy-optimization-to-group-relative-policy-optimization><svg viewBox="0 0 28 23" height="100%" width="19"><path d="M10 13a5 5 0 007.54.54l3-3a5 5 0 00-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/><path d="M14 11a5 5 0 00-7.54-.54l-3 3a5 5 0 007.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/></svg></a></span></h1><p>Finally, we arrive at Group Relative Policy Optimization (GRPO), the algorithm used to train Deepseek r1. This algorithm is used strictly in language modeling, so the ability to update before the end of a sequence doesn&rsquo;t really matter; all that matters is the stability and efficiency. Unlike prior algorithms, GRPO&rsquo;s main contribution isn&rsquo;t that it further stabilizes learning, but that it reduces memory and computation requirements drastically.</p><p>Turns out, for language modeling, we don&rsquo;t actually need the critic network, which saves us 50% of the memory and computation required to run reinforcement learning. We can just take all the other non-critic enhancements since REINFORCE and apply them in large batches , which is what GRPO does.</p>$$
\frac{1}{G}\sum_{i=1}^G \frac{1}{|o_i|} \sum_{t=1}^{|o_i|} \min \left[ \frac{\pi_\theta(o_{i,t}|q, o_{i,\lt t})}{\pi_{\theta_{\text{old}}}(o_{i,t}|q, o_{i,\lt t})} \hat{A}_{i,t}, \text{clip}\left(\frac{\pi_\theta(o_{i,t}|q, o_{i,\lt t})}{\pi_{\theta_{\text{old}}}(o_{i,t}|q, o_{i,\lt t})}, 1-\epsilon, 1+\epsilon\right) \hat{A}_{i,t} \right] - \beta D_{\text{KL}}\left[\pi_\theta \, \| \, \pi_{\text{ref}}
\right]
$$<p>You can see the lineage from PPO here. $q$ refers to a question, and $o$ here refers to an output sequence. $G$ is just the number of outputs in a a group. The main difference from PPO is that, instead of computing the advantage $A_t$ using a critic network, we now compute it within a group of sequences:</p>$$
\hat{A}_{i,t} = \frac{r_{i,t} - \text{mean}(\mathbf{r})}{\text{std}(\mathbf{r})}
$$<p>where $\mathbf{r}$ is the set of returns from the group of sequences. This replaces our neural estimate with an empirical estimate, while maintaining the benefits of the critic&rsquo;s stability.</p><p>There&rsquo;s a KL-divergence term at the end $\beta D_{\text{KL}}\left[\pi_\theta , | , \pi_{\text{ref}}\right]$, which is ensures that the policy doesn&rsquo;t change too much from the fine-tuned policy, $\pi_{\text{ref}}$.</p><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>I know I&rsquo;m pretty loose with the notations&ndash;I prefer this to adding a ton of extra symbols that don&rsquo;t contribute much.&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><div class=post-info><div class="post-date dt-published">2025-02-21</div><a class="post-hidden-url u-url" href=../../blog/from-reinforce-to-r1-an-abridged-genealogy/>/blog/from-reinforce-to-r1-an-abridged-genealogy/</a>
<a href=../../ class="p-name p-author post-hidden-author h-card" rel=me>jarbus</a><div class=post-taxonomies><ul class=post-tags><li><a href=../../tags/ai/>#Ai</a></li></ul></div></div></article><h3 class=read-next-title>Read next</h3><ul class=read-next-posts><li><a href=../../blog/the-penultimate-wave-of-ai/>The Penultimate Wave of AI</a></li><li><a href=../../blog/upwards-pressure-on-originality/>Originality in the Age of AI</a></li><li><a href=../../blog/emergent-trade/>Emergent Trade and Tolerated Theft Using Multi-Agent Reinforcement Learning</a></li><li><a href=../../blog/ai-index/>AI Index</a></li><li><a href=../../blog/tesla-and-false-advertising-in-ai/>Tesla and False Advertising in AI</a></li></ul><div class="pagination post-pagination"><div class="left pagination-item disabled"></div><div class="right pagination-item"><a href=../../blog/story-of-civilization-1-quotes/>Eloquence and Wit from Will Durant</a></div></div></main><footer class=common-footer><div class=common-footer-bottom><script>document.addEventListener("DOMContentLoaded",function(){document.documentElement.setAttribute("data-theme","dark"),showContent()});function showContent(){document.body.style.visibility="visible",document.body.style.opacity=1}</script></div><p class="h-card vcard"><a href=../../ class="p-name u-url url fn" rel=me>jarbus</a>
/
<a class="p-email u-email email" rel=me href=mailto:jarbus@tutanota.com>jarbus@tutanota.com</a></p></footer></div></body></html>