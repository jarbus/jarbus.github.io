<!doctype html><html lang=en-us data-theme><head><link rel=preload href=../../unifont-small.woff2 as=font type=font/woff2 crossorigin=anonymous><meta charset=utf-8><meta name=HandheldFriendly content="True"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer-when-downgrade"><a rel=me href=https://fosstodon.org/@jarbus></a><title>Low-Rank Factorizations are Indirect Encodings for Deep Neuroevolution -</title><meta name=description content="My latest paper is available on arxiv: Low Rank Factorizations are Indirect Encodings for Deep Neuroevolution.
The general idea is that we can search for stronger neural networks in a gradient-free fashion by restricting search to networks of low-rank. We show that it works well for language modeling and reinforcement learning tasks. It&rsquo;s essentially a crossover between the following papers:

LoRA: Low-Rank Adaptation of Large Language Models
Deep Neuroevolution: Genetic Algorithms Are a Competitive Alternative for Training Deep Neural Networks for Reinforcement Learning.

I&rsquo;ll be presenting it virtually for the Neuroevolution@Work workshop at GECCO 2025."><link rel=icon type=image/png href=../../favicon.png><link rel=apple-touch-icon-precomposed href=../../favicon.png><style>body{visibility:hidden;opacity:0}</style><noscript><style>body{visibility:visible;opacity:1}</style></noscript><link rel=stylesheet href=../../css/style.min.589470803b546b9b2dee7de3e6248c32a57ff907b44666dfb4f53a1613075f97.css integrity="sha256-WJRwgDtUa5st7n3j5iSMMqV/+Qe0RmbftPU6FhMHX5c="><script src=../../js/script.min.74bf1a3fcf1af396efa4acf3e660e876b61a2153ab9cbe1893ac24ea6d4f94ee.js type=text/javascript integrity="sha256-dL8aP88a85bvpKzz5mDodrYaIVOrnL4Yk6wk6m1PlO4="></script><script src=../../js/script.min.831b464bf6439bc0d69db2939710bb76d9d57d7772b5e86e8e7f1901569ada67.js type=text/javascript integrity="sha256-gxtGS/ZDm8DWnbKTlxC7dtnVfXdytehujn8ZAVaa2mc="></script><script src=../../js/script.min.a3d79dd06cfb7febed06f1f707b2cd0959edd8cdeab291d37d7eb9ac1a93a04a.js type=text/javascript integrity="sha256-o9ed0Gz7f+vtBvH3B7LNCVnt2M3qspHTfX65rBqToEo="></script><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}},window.addEventListener("load",e=>{document.querySelectorAll("mjx-container").forEach(function(e){e.parentElement.classList+="has-jax"})})</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script></head><body><a class=skip-main href=#main>Skip to main content</a><div class=container><header class=common-header><div class=header-top><h1 class=site-title><a href=../../></a></h1></div><img src=../../dithered-tree-contrast.gif alt="Dithered Tree" style="display:block;margin:0 auto;max-height:20vh"><nav><a href=../../ title=home>~jarbus</a>
<a href=../../blog/ title=posts>posts</a>
<a href=../../list title=list>list</a>
<a href=../../index.xml title=Rss>rss</a></nav></header><main id=main tabindex=-1><article class="post h-entry"><div class=post-header><header><h1 class="p-name post-title">LOW-RANK FACTORIZATIONS ARE INDIRECT ENCODINGS FOR DEEP NEUROEVOLUTION</h1></header></div><aside></aside><div class="content e-content"><p>My latest paper is available on arxiv: <a href=https://arxiv.org/abs/2504.03037>Low Rank Factorizations are Indirect Encodings for Deep Neuroevolution</a>.</p><p>The general idea is that we can search for stronger neural networks in a gradient-free fashion by restricting search to networks of low-rank. We show that it works well for language modeling and reinforcement learning tasks. It&rsquo;s essentially a crossover between the following papers:</p><ul><li><a href=https://arxiv.org/abs/2106.09685>LoRA: Low-Rank Adaptation of Large Language Models</a></li><li><a href=https://arxiv.org/abs/1712.06567>Deep Neuroevolution: Genetic Algorithms Are a Competitive Alternative for Training Deep Neural Networks for Reinforcement Learning</a>.</li></ul><p>I&rsquo;ll be presenting it virtually for the <a href=https://newk-gecco.github.io/>Neuroevolution@Work</a> workshop at <a href=https://gecco-2025.sigevo.org/HomePage>GECCO 2025</a>.</p></div><div class=post-info><div class="post-date dt-published">2025-05-29</div><a class="post-hidden-url u-url" href=../../blog/neuroevo-lora/>/blog/neuroevo-lora/</a>
<a href=../../ class="p-name p-author post-hidden-author h-card" rel=me>jarbus</a><div class=post-taxonomies><ul class=post-tags><li><a href=../../tags/ai/>#Ai</a></li></ul></div></div></article><h3 class=read-next-title>Read next</h3><ul class=read-next-posts><li><a href=../../blog/from-reinforce-to-r1-an-abridged-genealogy/>From REINFORCE to R1: an Abridged Genealogy of Reinforcement Learning</a></li><li><a href=../../blog/the-penultimate-wave-of-ai/>The Penultimate Wave of AI</a></li><li><a href=../../blog/upwards-pressure-on-originality/>Originality in the Age of AI</a></li><li><a href=../../blog/emergent-trade/>Emergent Trade and Tolerated Theft Using Multi-Agent Reinforcement Learning</a></li><li><a href=../../blog/ai-index/>AI Index</a></li></ul><div class="pagination post-pagination"><div class="left pagination-item"><a href=../../blog/everything-matters/>Everything Matters</a></div><div class="right pagination-item"><a href=../../blog/to-the-students/>To the Students</a></div></div></main><footer class=common-footer><div class=common-footer-bottom><script>document.addEventListener("DOMContentLoaded",function(){document.documentElement.setAttribute("data-theme","dark"),showContent()});function showContent(){document.body.style.visibility="visible",document.body.style.opacity=1}</script></div><p class="h-card vcard"><a href=../../ class="p-name u-url url fn" rel=me>jarbus</a>
/
<a class="p-email u-email email" rel=me href=mailto:jarbus@tutanota.com>jarbus@tutanota.com</a></p></footer></div></body></html>