<!DOCTYPE html>


    

<html lang="en-us" data-theme="dark">
<head>
    
        
<meta charset="utf-8">
<meta name="HandheldFriendly" content="True">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="referrer" content="no-referrer-when-downgrade">

<title>Numerical Stability in Flash Attention - </title>

<meta name="description" content="Flash attention, a recent implementation of attention which makes less calls to high-bandwidth memory, uses a version of the softmax function which is numerically stable. In this post, I&rsquo;ll briefly showcase how this is done and an example of an unstable softmax.
The softmax function is used in machine learning to convert a vector of real numbers to a vector of probabilities which sum to 1, and is defined as:">





<link rel="icon" type="image/x-icon" href="favicon.ico">
<link rel="apple-touch-icon-precomposed" href="favicon.png">


<style>
  body {
    visibility: hidden;
    opacity: 0;
  }
</style>

<noscript>
  <style>
    body {
      visibility: visible;
      opacity: 1;
    }
  </style>
</noscript>



    





    
    
        
    
    

    
        <link rel="stylesheet" href="../../css/style.min.ba6908e5362fa34b46234bdd9f3104d2bb79b25e6e0845585780d1dc45cdc05f.css" integrity="sha256-umkI5TYvo0tGI0vdnzEE0rt5sl5uCEVYV4DR3EXNwF8=">
    





    

    





    
    
        
    
    

    
        <script src="../../js/script.min.74bf1a3fcf1af396efa4acf3e660e876b61a2153ab9cbe1893ac24ea6d4f94ee.js" type="text/javascript" charset="utf-8" integrity="sha256-dL8aP88a85bvpKzz5mDodrYaIVOrnL4Yk6wk6m1PlO4="></script>
    







<meta property="og:title" content="Numerical Stability in Flash Attention" />
<meta property="og:description" content="Flash attention, a recent implementation of attention which makes less calls to high-bandwidth memory, uses a version of the softmax function which is numerically stable. In this post, I&rsquo;ll briefly showcase how this is done and an example of an unstable softmax.
The softmax function is used in machine learning to convert a vector of real numbers to a vector of probabilities which sum to 1, and is defined as:" />
<meta property="og:type" content="article" />
<meta property="og:url" content="/blog/numerical-stability-in-flash-attention/" /><meta property="article:section" content="blog" />
<meta property="article:published_time" content="2023-07-27T14:21:08-04:00" />
<meta property="article:modified_time" content="2023-07-27T14:21:08-04:00" />


<meta name="twitter:card" content="summary"/><meta name="twitter:title" content="Numerical Stability in Flash Attention"/>
<meta name="twitter:description" content="Flash attention, a recent implementation of attention which makes less calls to high-bandwidth memory, uses a version of the softmax function which is numerically stable. In this post, I&rsquo;ll briefly showcase how this is done and an example of an unstable softmax.
The softmax function is used in machine learning to convert a vector of real numbers to a vector of probabilities which sum to 1, and is defined as:"/>











    
</head>
<body>
    <a class="skip-main" href="#main">Skip to main content</a>
    <div class="container">
        <header class="common-header"> 
            
                <div class="header-top">
    <h1 class="site-title">
    <a href="../../"></a>
</h1>
    <ul class="social-icons">





</ul>
</div>

    <nav>
        
        
        <a class="" href="../../" title="home"> ~jarbus</a>
        
        <a class="" href="../../blog/" title="posts">posts</a>
        
        <a class="" href="../../index.xml" title="Rss">rss</a>
        
    </nav>






            
        </header>
        <main id="main" tabindex="-1"> 
            
    

    <article class="post h-entry">
        <div class="post-header">
            <header>
                <h1 class="p-name post-title">Numerical Stability in Flash Attention</h1>

                
            </header>
        </div>
        <div class="content e-content">
            <p><a href="https://hazyresearch.stanford.edu/blog/2023-01-12-flashattention-long-sequences">Flash attention</a>, a recent implementation of attention which makes less calls
to high-bandwidth memory, uses a version of the softmax function which is numerically stable. In this post, I&rsquo;ll briefly showcase how this is done and an example of an unstable softmax.</p>
<p>The softmax function is used in machine learning to convert a vector of
real numbers to a vector of probabilities which sum to 1, and is defined as:</p>
<pre><code>softmax(x) = [exp(x[i]) / sum([exp(xj) for xj in x]) for xi in x]
</code></pre>
<p>where x is a vector of real numbers.</p>
<p>The python implementation below is numerically unstable because it involves
exponentiation of large numbers, which can lead to overflow. Crucially,
underflow is not an issue, because exp(x) approaches zero when x is a large
negative number.</p>
<div class="highlight"><pre tabindex="0" style="color:#fff;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#b6a0ff">import</span> numpy <span style="color:#b6a0ff">as</span> np
</span></span><span style="display:flex;"><span><span style="color:#b6a0ff">def</span> <span style="color:#feacd0">unstable_softmax</span>(x):
</span></span><span style="display:flex;"><span>    fx_unstable <span style="color:#00d3d0">=</span> np<span style="color:#00d3d0">.</span>exp(x)
</span></span><span style="display:flex;"><span>    <span style="color:#b6a0ff">return</span> fx_unstable <span style="color:#00d3d0">/</span> np<span style="color:#00d3d0">.</span>sum(fx_unstable)
</span></span></code></pre></div><p>The following implementation is stable however, because there is no
exponentiation of large numbers:</p>
<div class="highlight"><pre tabindex="0" style="color:#fff;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#b6a0ff">def</span> <span style="color:#feacd0">stable_softmax</span>(x):
</span></span><span style="display:flex;"><span>    fx_stable <span style="color:#00d3d0">=</span> x <span style="color:#00d3d0">-</span> np<span style="color:#00d3d0">.</span>max(x)
</span></span><span style="display:flex;"><span>    <span style="color:#b6a0ff">return</span> np<span style="color:#00d3d0">.</span>exp(fx_stable) <span style="color:#00d3d0">/</span> np<span style="color:#00d3d0">.</span>sum(np<span style="color:#00d3d0">.</span>exp(fx_stable))
</span></span></code></pre></div><p>Instead, the max of the vector is subtracted from each element. This does not
change the result of the softmax after the division as this subtraction is also
performed in the denominator, thus cancelling out.</p>
<p>Let&rsquo;s compare the two implementations:</p>
<div class="highlight"><pre tabindex="0" style="color:#fff;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#00d3d0">&gt;&gt;&gt;</span> a <span style="color:#00d3d0">=</span> np<span style="color:#00d3d0">.</span>array([<span style="color:#00bcff">6.0</span>, <span style="color:#00d3d0">-</span><span style="color:#00bcff">3</span>, <span style="color:#00bcff">15</span>], dtype<span style="color:#00d3d0">=</span>np<span style="color:#00d3d0">.</span>float32)
</span></span><span style="display:flex;"><span><span style="color:#00d3d0">&gt;&gt;&gt;</span> stable_softmax(a)
</span></span><span style="display:flex;"><span>[<span style="color:#00bcff">1.2339458e-04</span> <span style="color:#00bcff">1.5228101e-08</span> <span style="color:#00bcff">9.9987662e-01</span>]
</span></span><span style="display:flex;"><span><span style="color:#00d3d0">&gt;&gt;&gt;</span> unstable_softmax(a)
</span></span><span style="display:flex;"><span>[<span style="color:#00bcff">1.2339458e-04</span> <span style="color:#00bcff">1.5228101e-08</span> <span style="color:#00bcff">9.9987656e-01</span>]
</span></span></code></pre></div><p>As you can see, the results are mostly equal, save for a few digits.
Now let&rsquo;s look at what happens with 16 bits of precision:</p>
<div class="highlight"><pre tabindex="0" style="color:#fff;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#00d3d0">&gt;&gt;&gt;</span> a <span style="color:#00d3d0">=</span> np<span style="color:#00d3d0">.</span>array([<span style="color:#00bcff">6.0</span>, <span style="color:#00d3d0">-</span><span style="color:#00bcff">3</span>, <span style="color:#00bcff">15</span>], dtype<span style="color:#00d3d0">=</span>np<span style="color:#00d3d0">.</span>float16)
</span></span><span style="display:flex;"><span><span style="color:#00d3d0">&gt;&gt;&gt;</span> stable_softmax(a)
</span></span><span style="display:flex;"><span>[ <span style="color:#00bcff">0.</span>  <span style="color:#00bcff">0.</span> nan]
</span></span><span style="display:flex;"><span><span style="color:#00d3d0">&gt;&gt;&gt;</span> unstable_softmax(a)
</span></span><span style="display:flex;"><span>[ <span style="color:#00bcff">1.234e-04</span> <span style="color:#00bcff">0.000e+00</span> <span style="color:#00bcff">1.000e+00</span>]
</span></span></code></pre></div><p>When working with 16 bits of precision, we observe that exp(15) produces a numerical overflow
which turns the third element into a NaN. This is because exp(15) produces a value that can
not be represented by a float16.</p>
<p>To recap, we showed that softmax is numerically unstable, especially when working with small precision bits. Because softmax uses exponentials in the numerator and denominator, we can subtract all exponents by the maximum exponent, constraining all the values between 0 and 1 and preventing numerical overflow.</p>

        </div>
        

    


<div class="post-info">
    
        <div class="post-date dt-published">2023-07-27</div>
    

    <a class="post-hidden-url u-url" href="../../blog/numerical-stability-in-flash-attention/">/blog/numerical-stability-in-flash-attention/</a>
    <a href="" class="p-name p-author post-hidden-author h-card" rel="me">jarbus</a>


    <div class="post-taxonomies">
        
            
                <ul class="post-tags">
                    
                        
                        <li><a href="../../tags/programming/">#programming</a></li>
                    
                </ul>
        
    </div>
</div>

    </article>

    
        
        
            <h3 class="read-next-title">Read next</h3>
            <ul class="read-next-posts">
                
                <li><a href="../../blog/introducing-kittyplot/">Introducing Kittyplot</a></li>
                
                <li><a href="../../blog/unexpected-benefits-of-testing/">Unexpected Benefits of Testing Code</a></li>
                
            </ul>
        
    

    
        
    <div class="pagination post-pagination">
        <div class="left pagination-item disabled">
            
        </div>
        <div class="right pagination-item ">
            
                <a href="../../blog/introducing-kittyplot/">Introducing Kittyplot</a>
            
        </div>
    </div>




    

    
        







    

        </main>
        
            <footer class="common-footer">
    
    

    <div class="common-footer-bottom">
        
        
        
        
        
        
        

        

    




<script>
const STORAGE_KEY = 'user-color-scheme'
const defaultTheme = "dark-without-switcher"

let currentTheme
let switchButton
let autoDefinedScheme = window.matchMedia('(prefers-color-scheme: dark)')

const autoChangeScheme = e => {
    currentTheme = e.matches ? 'dark' : 'light'
    document.documentElement.setAttribute('data-theme', currentTheme)
    changeButtonText()
}

document.addEventListener('DOMContentLoaded', function() {
    switchButton = document.querySelector('.theme-switcher')
    currentTheme = detectCurrentScheme()
    if (currentTheme == 'dark') {
        document.documentElement.setAttribute('data-theme', 'dark')
    }
    if (currentTheme == 'auto') {
        autoChangeScheme(autoDefinedScheme);
        autoDefinedScheme.addListener(autoChangeScheme);
    }

    if (switchButton) {
        changeButtonText()
        switchButton.addEventListener('click', switchTheme, false)
    }
  
    showContent()
})

function detectCurrentScheme() {
    if (localStorage.getItem(STORAGE_KEY)) {
        return localStorage.getItem(STORAGE_KEY)
    } 
    if (defaultTheme) {
        return defaultTheme
    } 
    if (!window.matchMedia) {
        return 'light'
    } 
    if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        return 'dark'
    }
    return 'light'
}

function changeButtonText()
{   
    if (switchButton) {
        switchButton.textContent = currentTheme == 'dark' ?  "Light theme" : "Dark theme"
    }
}

function switchTheme(e) {
    if (currentTheme == 'dark') {
        localStorage.setItem(STORAGE_KEY, 'light')
        document.documentElement.setAttribute('data-theme', 'light')
        currentTheme = 'light'
    } else {
        localStorage.setItem(STORAGE_KEY, 'dark')
        document.documentElement.setAttribute('data-theme', 'dark')
        currentTheme = 'dark'
    }
    changeButtonText()
}

function showContent() {
    document.body.style.visibility = 'visible';
    document.body.style.opacity = 1;
}
</script>   
    </div>

    <p class="h-card vcard">

    <a href=ZgotmplZ class="p-name u-url url fn" rel="me">jarbus</a> 

     
        /
        <a class="p-email u-email email" rel="me" href="mailto:jarbus@tutanota.com">jarbus@tutanota.com</a>
    

     
        <img class="u-photo" src="../../images/me.png" />
    
</p> 
</footer>

        
    </div>
</body>
</html>
