<!doctype html><html lang=en-us data-theme><head><link rel=preload href=../../unifont-small.woff2 as=font type=font/woff2 crossorigin=anonymous><meta charset=utf-8><meta name=HandheldFriendly content="True"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer-when-downgrade"><a rel=me href=https://fosstodon.org/@jarbus></a><title>Numerical Stability in Flash Attention -</title><meta name=description content="Flash attention, a recent implementation of attention which makes less calls
to high-bandwidth memory, uses a version of the softmax function which is numerically stable. In this post, I&rsquo;ll briefly showcase how this is done and an example of an unstable softmax.
The softmax function is used in machine learning to convert a vector of
real numbers to a vector of probabilities which sum to 1, and is defined as:"><link rel=icon type=image/png href=../../favicon.png><link rel=apple-touch-icon-precomposed href=../../favicon.png><style>body{visibility:hidden;opacity:0}</style><noscript><style>body{visibility:visible;opacity:1}</style></noscript><link rel=stylesheet href=../../css/style.min.df7ab593181f6eb095bf3467d78b611aae723a1a52787089b45a198b6e0f43cf.css integrity="sha256-33q1kxgfbrCVvzRn14thGq5yOhpSeHCJtFoZi24PQ88="><script src=../../js/script.min.74bf1a3fcf1af396efa4acf3e660e876b61a2153ab9cbe1893ac24ea6d4f94ee.js type=text/javascript integrity="sha256-dL8aP88a85bvpKzz5mDodrYaIVOrnL4Yk6wk6m1PlO4="></script><script src=../../js/script.min.cef56383505bdd943196b6090ac7df0a357e5f68da0e6b85a559fc035e9df26a.js type=text/javascript integrity="sha256-zvVjg1Bb3ZQxlrYJCsffCjV+X2jaDmuFpVn8A16d8mo="></script><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}},window.addEventListener("load",e=>{document.querySelectorAll("mjx-container").forEach(function(e){e.parentElement.classList+="has-jax"})})</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script></head><body><a class=skip-main href=#main>Skip to main content</a><div class=container><header class=common-header><div class=header-top><h1 class=site-title><a href=../../></a></h1></div><img src=../../dithered-tree-contrast.gif alt="Dithered Tree" style="display:block;margin:0 auto;max-height:20vh"><nav><a href=../../ title=home>~jarbus</a>
<a href=../../blog/ title=posts>posts</a>
<a href=../../list title=list>list</a>
<a href=../../index.xml title=Rss>rss</a></nav></header><main id=main tabindex=-1><article class="post h-entry"><div class=post-header><header><h1 class="p-name post-title">NUMERICAL STABILITY IN FLASH ATTENTION</h1></header></div><div class="content e-content"><p><a href=https://hazyresearch.stanford.edu/blog/2023-01-12-flashattention-long-sequences>Flash attention</a>, a recent implementation of attention which makes less calls
to high-bandwidth memory, uses a version of the softmax function which is numerically stable. In this post, I&rsquo;ll briefly showcase how this is done and an example of an unstable softmax.</p><p>The softmax function is used in machine learning to convert a vector of
real numbers to a vector of probabilities which sum to 1, and is defined as:</p><pre><code>softmax(x) = [exp(x[i]) / sum([exp(xj) for xj in x]) for xi in x]
</code></pre><p>where x is a vector of real numbers.</p><p>The python implementation below is numerically unstable because it involves
exponentiation of large numbers, which can lead to overflow. Crucially,
underflow is not an issue, because exp(x) approaches zero when x is a large
negative number.</p><div class=highlight><pre tabindex=0 style=color:#fff;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#b6a0ff>import</span> numpy <span style=color:#b6a0ff>as</span> np
</span></span><span style=display:flex><span><span style=color:#b6a0ff>def</span> <span style=color:#feacd0>unstable_softmax</span>(x):
</span></span><span style=display:flex><span>    fx_unstable <span style=color:#00d3d0>=</span> np<span style=color:#00d3d0>.</span>exp(x)
</span></span><span style=display:flex><span>    <span style=color:#b6a0ff>return</span> fx_unstable <span style=color:#00d3d0>/</span> np<span style=color:#00d3d0>.</span>sum(fx_unstable)
</span></span></code></pre></div><p>The following implementation is stable however, because there is no
exponentiation of large numbers:</p><div class=highlight><pre tabindex=0 style=color:#fff;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#b6a0ff>def</span> <span style=color:#feacd0>stable_softmax</span>(x):
</span></span><span style=display:flex><span>    fx_stable <span style=color:#00d3d0>=</span> x <span style=color:#00d3d0>-</span> np<span style=color:#00d3d0>.</span>max(x)
</span></span><span style=display:flex><span>    <span style=color:#b6a0ff>return</span> np<span style=color:#00d3d0>.</span>exp(fx_stable) <span style=color:#00d3d0>/</span> np<span style=color:#00d3d0>.</span>sum(np<span style=color:#00d3d0>.</span>exp(fx_stable))
</span></span></code></pre></div><p>Instead, the max of the vector is subtracted from each element. This does not
change the result of the softmax after the division as this subtraction is also
performed in the denominator, thus cancelling out.</p><p>Let&rsquo;s compare the two implementations:</p><div class=highlight><pre tabindex=0 style=color:#fff;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#00d3d0>&gt;&gt;&gt;</span> a <span style=color:#00d3d0>=</span> np<span style=color:#00d3d0>.</span>array([<span style=color:#00bcff>6.0</span>, <span style=color:#00d3d0>-</span><span style=color:#00bcff>3</span>, <span style=color:#00bcff>15</span>], dtype<span style=color:#00d3d0>=</span>np<span style=color:#00d3d0>.</span>float32)
</span></span><span style=display:flex><span><span style=color:#00d3d0>&gt;&gt;&gt;</span> stable_softmax(a)
</span></span><span style=display:flex><span>[<span style=color:#00bcff>1.2339458e-04</span> <span style=color:#00bcff>1.5228101e-08</span> <span style=color:#00bcff>9.9987662e-01</span>]
</span></span><span style=display:flex><span><span style=color:#00d3d0>&gt;&gt;&gt;</span> unstable_softmax(a)
</span></span><span style=display:flex><span>[<span style=color:#00bcff>1.2339458e-04</span> <span style=color:#00bcff>1.5228101e-08</span> <span style=color:#00bcff>9.9987656e-01</span>]
</span></span></code></pre></div><p>As you can see, the results are mostly equal, save for a few digits.
Now let&rsquo;s look at what happens with 16 bits of precision:</p><div class=highlight><pre tabindex=0 style=color:#fff;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#00d3d0>&gt;&gt;&gt;</span> a <span style=color:#00d3d0>=</span> np<span style=color:#00d3d0>.</span>array([<span style=color:#00bcff>6.0</span>, <span style=color:#00d3d0>-</span><span style=color:#00bcff>3</span>, <span style=color:#00bcff>15</span>], dtype<span style=color:#00d3d0>=</span>np<span style=color:#00d3d0>.</span>float16)
</span></span><span style=display:flex><span><span style=color:#00d3d0>&gt;&gt;&gt;</span> unstable_softmax(a)
</span></span><span style=display:flex><span>[ <span style=color:#00bcff>0.</span>  <span style=color:#00bcff>0.</span> nan]
</span></span><span style=display:flex><span><span style=color:#00d3d0>&gt;&gt;&gt;</span> stable_softmax(a)
</span></span><span style=display:flex><span>[ <span style=color:#00bcff>1.234e-04</span> <span style=color:#00bcff>0.000e+00</span> <span style=color:#00bcff>1.000e+00</span>]
</span></span></code></pre></div><p>When working with 16 bits of precision, we observe that exp(15) produces a numerical overflow
which turns the third element into a NaN. This is because exp(15) produces a value that can
not be represented by a float16.</p><p>To recap, we showed that softmax is numerically unstable, especially when working with small precision bits. Because softmax uses exponentials in the numerator and denominator, we can subtract all exponents by the maximum exponent, constraining all the values between 0 and 1 and preventing numerical overflow.</p></div><div class=post-info><div class="post-date dt-published">2023-07-27</div><a class="post-hidden-url u-url" href=../../blog/numerical-stability-in-flash-attention/>/blog/numerical-stability-in-flash-attention/</a>
<a href=../../ class="p-name p-author post-hidden-author h-card" rel=me>jarbus</a><div class=post-taxonomies><ul class=post-tags><li><a href=../../tags/programming/>#Programming</a></li></ul></div></div></article><h3 class=read-next-title>Read next</h3><ul class=read-next-posts><li><a href=../../blog/introducing-kittyplot/>Introducing Kittyplot</a></li><li><a href=../../blog/unexpected-benefits-of-testing/>Unexpected Benefits of Testing Code</a></li></ul><div class="pagination post-pagination"><div class="left pagination-item"><a href=../../blog/the-way-of-zen/>The Way of Zen</a></div><div class="right pagination-item"><a href=../../blog/introducing-kittyplot/>Introducing Kittyplot</a></div></div></main><footer class=common-footer><div class=common-footer-bottom><script>document.addEventListener("DOMContentLoaded",function(){document.documentElement.setAttribute("data-theme","dark"),showContent()});function showContent(){document.body.style.visibility="visible",document.body.style.opacity=1}</script></div><p class="h-card vcard"><a href=../../ class="p-name u-url url fn" rel=me>jarbus</a>
/
<a class="p-email u-email email" rel=me href=mailto:jarbus@tutanota.com>jarbus@tutanota.com</a></p></footer></div></body></html>