<!DOCTYPE html> <html lang="en"> <head> <link rel="stylesheet" type="text/css" href="index.css"> <link href="https://fonts.googleapis.com/css?family=EB+Garamond:400" rel="stylesheet"> <title>AI Index</title> <meta name=viewport content="width=device-width, initial-scale=1.0" /> </head> <p id="header"><a id="menuitem" href="index">AI Index</a></p> <main> <hr width="80%"> <p id="menu" align="center"> <a id="menuitem" href="About">About</a> <a id="menuitem" href="Posts">Posts</a> <a id="menuitem" href="Contact">Contact</a></p>
<p>An ever-expanding list of concepts in the field of AI to give myself and others an easy reference. Each item in the list contains a short, rudimentary definition I’ve written, as well as a link to a resource that can explain it better.</p>
<ul>
<li><a href="https://mc.ai/advantage-function-in-deep-reinforcement-learning/">Advantage Function</a>: The difference between a Q-value for a state-action pair and a value for the state. Useful to determine how good an action is relative to its state.</li>
<li><a href="https://towardsdatascience.com/intuitive-understanding-of-attention-mechanism-in-deep-learning-6c9482aecf4f">Attention</a>: Neural networks are able to “pay attention” to specific parts of input or output, useful in translating languages or predicting sequences. For example, when trying to predict the next word in “the clouds in the”, you pay attention to the word <em>cloud</em> to predict the word <em>sky</em></li>
<li><a href="https://en.wikipedia.org/wiki/Autoencoder">Autoencoder</a><span class="math inline"><sup><em>W</em></sup></span>: A type of neural network that attempts to take raw data, convert it into a simpler representation (usually by limiting the amount of nodes in a hidden layer for representation), and then decode the representation back into it’s data. They are primarly used to extract the useful properties from data automatically. They can do this in an unsupervised fashion, since their output targets are the given input, requiring no labeling.</li>
<li><a href="https://en.wikipedia.org/wiki/Automated_machine_learning">AutoML</a><span class="math inline"><sup><em>W</em></sup></span>: Systems where the entire machine learning process, from data preparation to network design, is automated.</li>
<li><a href="https://en.wikipedia.org/wiki/Backpropagation">Backpropogation</a><span class="math inline"><sup><em>W</em></sup></span>: The algorithm and calculus behind gradient descent traditionally used in feed-forward neural networks</li>
<li><a href="https://en.wikipedia.org/wiki/Bayesian_statistics">Bayesian</a>: An interpretation of probability where the phrase “probability” expresses a degree of believe in an event. Uses expected values and random variables.</li>
<li><a href="https://en.wikipedia.org/wiki/Bootstrapping_(statistics)">Bootstrapping</a><span class="math inline"><sup><em>W</em></sup></span>: Sampling from a distribution to approximate a function. In cases of reinforcement learning, bootstrapping usually samples potential future values to approximate a current value.</li>
<li><a href="https://en.wikipedia.org/wiki/Convolutional_neural_network">Convolutional Neural Network (CNN)</a><span class="math inline"><sup><em>W</em></sup></span>: A neural network primarly used for image processing. These networks design filters for specific parts of an image to extract higher level information, filters such as detecting a certain type of edge.</li>
<li><a href="https://en.wikipedia.org/wiki/Data_mining">Data Mining</a>: Discovering knowledge and patterns from massive amounts of data, usually in an unsupervised fashion.</li>
<li><a href="https://en.wikipedia.org/wiki/Deep_learning">Deep Learning</a><span class="math inline"><sup><em>W</em></sup></span>: A subset of machine learning with a multi-step learning process, usually referring to neural networks with two or more layers.</li>
<li><a href="https://stats.stackexchange.com/questions/221402/understanding-the-role-of-the-discount-factor-in-reinforcement-learning">Discount Factors</a>: A variable (usually <span class="math inline"><em>γ</em></span>) that determines how much a model cares about rewards in the distant future compared to immediate rewards.
<ul>
<li><span class="math inline"><em>γ</em> = 0</span> cares only about immediate reward,</li>
<li><span class="math inline"><em>γ</em> = 1</span> cares only about sum of all future rewards.</li>
</ul></li>
<li><a href="https://towardsdatascience.com/eligibility-traces-in-reinforcement-learning-a6b458c019d6">Eligibility Trace</a>: For temporal learning, the eligibility trace is a vector of decaying values that represent when weights were last used. When we encounter an error, this vector allows us to update recent weights harder than weights used long ago.</li>
<li><a href="https://en.wikipedia.org/wiki/Feedforward_neural_network">Feed-forward Neural Network</a><span class="math inline"><sup><em>W</em></sup></span>: A simple neural network where information is passed strictly from one layer to the next.</li>
<li><a href="https://en.wikipedia.org/wiki/Generative_adversarial_network">Generative Adversarial Network (GAN)</a><span class="math inline"><sup><em>W</em></sup></span>: A set of two or more neural networks that can generate new data based on existing training data. A simple example is <a href="https://thispersondoesnotexist.com/">https://thispersondoesnotexist.com/</a>, that can generate fake pictures of humans.
<ul>
<li>GANs consist of one generator network with the goal to make realistic new data, and a distinction network with the goal to tell real data from fake. As the networks train, they each improve at their individual task, forcing their adversaries to improve in turn.</li>
</ul></li>
<li><a href="https://en.wikipedia.org/wiki/Genetic_algorithm">Genetic Algorithms</a><span class="math inline"><sup><em>W</em></sup></span>: Algorithms that try to mimic the evolutionary process by randomly modifying the best-performing sets of parameters while discarding those with the worst performance, then repeating.</li>
<li><a href="https://openai.com/blog/better-language-models/">GPT-2</a>: A language model from OpenAI that can generate text that mimics a writing style incredibly well.</li>
<li><a href="https://en.wikipedia.org/wiki/Gradient_descent">Gradient Descent</a><span class="math inline"><sup><em>W</em></sup></span>: An interative process that attempts to find a minimum of a function that works by moving in the direction that will decrease the gradient until a local minima is reached.</li>
<li><a href="https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning)">Hyperparameters</a><span class="math inline"><sup><em>W</em></sup></span>: Manually defined parameters of the model, such as the size of a neural network, or manually defined parameters of the machine learning algorithm, such as learning rate.</li>
<li><a href="https://dibyaghosh.com/blog/probability/kldivergence.html">KL Divergence</a>: Divergence between two distributions of data. Useful for determining how different fake data is from real data (GANs) or for determining how differemt two policies are for trust-based reinforcement learning.</li>
<li><a href="https://en.wikipedia.org/wiki/Self-driving_car">Level 0-5</a><span class="math inline"><sup><em>W</em></sup></span>: The different “levels” of autonomy related to self driving cars. 0 represents full human control while 5 represents full vehicle control.</li>
<li><a href="https://en.wikipedia.org/wiki/Long_short-term_memory">Long Short Term Memory (LSTM)</a><span class="math inline"><sup><em>W</em></sup></span>: A type of recurrent neural network that works exceptionally well with sequential input. A unique trait of these networks are their memory cells’ “forget gates”, which allow them to control how long they hold onto data for.</li>
<li><a href="https://en.wikipedia.org/wiki/Markov_decision_process">Markov Decision Process</a><span class="math inline"><sup><em>W</em></sup></span>: A system of states, actions, and probabilities of getting to other states given actions taken from previous states.</li>
<li><a href="https://en.wikipedia.org/wiki/Multi-armed_bandit">Multi-armed Bandit</a><span class="math inline"><sup><em>W</em></sup></span>: A core component of reinforcement learning, the multi-armed bandit problem is the classic “exploration versus exploitation” tradeoff. In this problem, expected gain must be maximized in an environment with varying rewards, forcing an agent to decide between keeping an option they know to be safe versus exploring new options that might be better.</li>
<li><a href="https://stats.stackexchange.com/questions/897/online-vs-offline-learning">Online/Offline Learning</a>: Online learning happens as data comes in. Offline learning happens after data is collected and made into a batch.</li>
<li><a href="http://mlss.tuebingen.mpg.de/2015/slides/ghahramani/gp-neural-nets15.pdf">Parametric &amp; Non-Parametric Models</a>: Parametric models use a finite number of parameters, like weights in linear regression, to represent a learned hypothesis. Non-Parametric models use variable/infinite/no parameters, like data points in nearest neighbors, to represent a learned hypothesis.</li>
<li><a href="https://en.wikipedia.org/wiki/Recurrent_neural_network">Recurrent Neural Network (RNN)</a><span class="math inline"><sup><em>W</em></sup></span>: A type of network that can store state, giving it a type of memory that can process a series of inputs. This can be accomplished by having a cycle within the network.</li>
<li><a href="https://en.wikipedia.org/wiki/Regression_analysis">Regression</a><span class="math inline"><sup><em>W</em></sup></span>: A set of models that determine relationships between data and a dependent value. For example, linear regression tries to approximate a dependent value while logistic regression tries to determine the probability of a dependent value.</li>
<li><a href="https://en.wikipedia.org/wiki/Residual_neural_network">Residual Neural Network</a><span class="math inline"><sup><em>W</em></sup></span>: Networks with connections that skip some layers and connect to non-adjacent ones. This type of network helps counter the vanishing gradient problem</li>
<li><a href="https://en.wikipedia.org/wiki/Reinforcement_learning">Reinforcement Learning</a><span class="math inline"><sup><em>W</em></sup></span>: Algorithms are given a world state they are able to interact with, and learn from the reward their interactions give them.
<ul>
<li><a href="https://ai.stackexchange.com/questions/4456/whats-the-difference-between-model-free-and-model-based-reinforcement-learning">Model-Based</a>: You create a model of the world and can predict what the next state and reward will be for each action</li>
<li><a href="https://ai.stackexchange.com/questions/4456/whats-the-difference-between-model-free-and-model-based-reinforcement-learning">Model-Free</a>: You know what action to take without knowing what to expect, since you don’t have a model of the world</li>
<li><a href="https://www.freecodecamp.org/news/an-intro-to-advantage-actor-critic-methods-lets-play-sonic-the-hedgehog-86d6240171d/">Value</a>: Networks that determine the value of a state.</li>
<li><a href="https://www.freecodecamp.org/news/an-intro-to-advantage-actor-critic-methods-lets-play-sonic-the-hedgehog-86d6240171d/">Policy</a>: Network to choose actions. Can directly optimize model instead of computing values, useful when you have a continuous action space. Only uses reward function. Requires a score function to evaluate performance of policy, usually total rewards accumulated given a period of time.</li>
<li><a href="https://www.freecodecamp.org/news/an-intro-to-advantage-actor-critic-methods-lets-play-sonic-the-hedgehog-86d6240171d/">Actor Critic</a>: Backbone of state of the art reinforcement learning algorithms.
<ul>
<li>Uses a value-based Critic to measure how good the action taken is</li>
<li>Uses a policy-based Actor to to choose actions</li>
</ul></li>
</ul></li>
<li><a href="https://en.wikipedia.org/wiki/Stochastic">Stochastic</a><span class="math inline"><sup><em>W</em></sup></span>: Randomly determined process, usually refers to probabilistic outputs of machine learning systems</li>
<li><a href="https://en.wikipedia.org/wiki/Supervised_learning">Supervised Learning</a><span class="math inline"><sup><em>W</em></sup></span>: A model learns to produce a desired output and knows what that output is. Example: Image Recognition</li>
<li><a href="https://en.wikipedia.org/wiki/Temporal_difference_learning">Temporal Difference</a><span class="math inline"><sup><em>W</em></sup></span>: Model-free reinforcement learning design which learns by bootstrapping value samples in order to approximate a value function. Once more information is revealed about the true value during later timesteps, you can update the low information bootstrapped guess by using the newly acquired outcome as a “ground truth” to train a network.
<ul>
<li>Temporal Difference error is the difference between consecutive temporal predictions.</li>
</ul></li>
<li><a href="https://ai.stackexchange.com/questions/7359/what-is-a-trajectory-in-reinforcement-learning">Trajectories</a>: The history of states (and potentially actions) taken during a walk of a Markov Decision Process.</li>
<li><a href="https://en.wikipedia.org/wiki/Transfer_learning">Transfer Learning</a><span class="math inline"><sup><em>W</em></sup></span>: Taking parts of a network trained on one data set, and using it in a different network with a different dataset.</li>
<li><a href="https://towardsdatascience.com/transformers-141e32e69591">Transformer</a>: A type of recurrent neural network primarily used with sequential data, like language translation. These networks use an attention model to improve performance.</li>
<li><a href="https://en.wikipedia.org/wiki/Unsupervised_learning">Unsupervised Learning</a><span class="math inline"><sup><em>W</em></sup></span>: A model learns to produce a desired output without being told what it’s looking for. Example: Playing Chess</li>
<li><a href="https://en.wikipedia.org/wiki/Vanishing_gradient_problem">Vanishing Gradient Problem</a><span class="math inline"><sup><em>W</em></sup></span>: In a network, a gradient can become vanishingly small which will stop the weight from changing it’s value, since weights are modified based on their contribution to the gradient.</li>
</ul>
<p>Feel free to contact me with any suggested additions/changes at <a href="mailto:jarbus@tutanota.com">jarbus@tutanota.com</a>.</p>
</main></html>
